%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% This file is part of nsCouette -- A high-performance code for direct         !
% numerical simulations of turbulent Taylor-Couette flow                       !
%                                                                              !
% Copyright (C) 2019 Marc Avila, Bjoern Hof, Jose Manuel Lopez, Markus Rampp,  !
%                    Liang Shi, Alberto Vela-Martin, Daniel Feldmann.          !
%                                                                              !
% nsCouette is free software: you can redistribute it and/or modify it under   !
% the terms of the GNU General Public License as published by the Free         !
% Software Foundation, either version 3 of the License, or (at your option)    !
% any later version.                                                           !
%                                                                              !
% nsCouette is distributed in the hope that it will be useful, but WITHOUT ANY !
% WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS    !
% FOR A PARTICULAR PURPOSE. See the GNU General Public License for more        !
% details.                                                                     !
%                                                                              !
% You should have received a copy of the GNU General Public License along with !
% nsCouette. If not, see <http://www.gnu.org/licenses/>.                       !
%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

\documentclass[a4paper, 11pt, DIV=11]{scrartcl}

\input{userDefs/myPreamble}
\input{userDefs/myCommands}

\title{A user guide for \nsc}
\author{Daniel~Feldmann, Jose~Manuel~L\'opez,\\ Markus~Rampp, Liang~Shi \& Marc~Avila}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This is a hands-on introduction to \nsc; A highly scalable software tool
to integrate the full Navier--Stokes equations for incompressible fluid
flows between deferentially heated and independently rotating concentric
cylinders forward in time. It is based on a pseudospectral spatial
discretisation and dynamic time-stepping. It is implemented in modern
\fortran with a hybrid \mpi-\omp parallelisation scheme and thus tailored
to compute turbulent flows at high Reynolds and Rayleigh numbers. With a
number of pre-configured \code{Makefiles} for different architectures,
easy installation, simple handling of input parameters, portable
\code{HDF5} output, visualisation tools, comprehensive documentation and
internal quality assurance, \nsc is designed to lower the barrier to entry
to numerical research in highly-turbulent fluid flows.
\par
An additional GPU-accelerated implementation (\cuda) for intermediate problem
sizes as well as a basic version for turbulent pipe flow (\nsp) are also
provided. This guide can be used to get familiar with the prerequisites,
compilation, running and structure of our code before using it for your
own research project.
\end{abstract}
% Text width: \printinunitsof{mm}\prntlen{\textwidth}\\
\tableofcontents

\section{For the impatient}
\label{sec:forTheImpatient}

Basically, downloading, building, and running the code goes as easy as this:
\begin{lstlisting}[language=bash]
git clone https://github.com/dfeldmann/nsCouette
make ARCH=make.arch.gcc-linux HDF5IO=no
export OMP_NUM_THREADS=4
mpiexec -np 2 ./nsCouette.x < nsCouette.in
\end{lstlisting}
However, for further details and explanations, examples and problem solving
please take the time to read this document and follow the provided tutorials.
If this doesn't help, feel free to \href{mailto:nsCouette@zarm.uni-bremen.de}{contact us}.
We are happy to help and we hope you enjoy using \nsc!

\section{About Taylor-Couette}
\label{sec:taylorCouette}

The Taylor-Couette (TC) set-up is one of the most famous paradigmatic systems
for wall-bounded shear flows in general and maybe \text{the} most important
one if you are interested in rotating shear flows in particular. For
understanding the following chapters of this documentation, it might help
to acquaint yourself with a bit of crucial terminology and basic
concepts of the TC system as well as with the notation we will use throughout
this documentation and in the source code of \nsc.
\par
Here and there throughout the manual, we will mention how the code can be used to
include thermal convection in the TC system (Section~\ref{sec:thermalConvection})
or change to a straight pipe flow set-up (Section~\ref{sec:nsPipe}), the other very
important canonical shear flow system in cylindrical co-ordinates.

\subsection{Governing equations}
\label{sec:governingEquations}

We consider a fluid with constant properties (density $\rho$ and kinematic
viscosity $\nu$) confined between two concentric cylinders as shown in
figure~\ref{fig:tc}.
\begin{figure}
\includegraphics[width=1.00\linewidth]{figures/taylorCouette.png}
\caption{Schematic of the Taylor-Couette (TC) system in a cylindrical
co-ordinate frame work ($r, \theta, z$) and its relevant properties:
Radius of the inner/outer cylinder wall ($r_{i,o}$), height of the
computational domain ($L_z$), angular rotation speed of the inner/outer
cylinder walls ($\Omega_{i,o}$). Sketch taken from our CAF paper~\cite{Shi2015}.
% \df{Prepare a nicer sketch when we are done with everything else\dots}
}
\label{fig:tc}
\end{figure}
Rotating at least one of the cylinders causes a motion of the confined fluid;
A fluid-dynamical system well-known as Taylor-Couette (TC) flow. The so
generated fluid motion is governed by the incompressible Navier-Stokes equations
\begin{align}
\nabla\cdot\vec{u} = 0
\hspace{1em}\text{and}\hspace{1em}
\frac{\partial\vec{u}}{\partial t} + \vec{u}\cdot\nabla\vec{u} =
\frac{\nabla p^{\ast}}{\rho} + \nu\laplacian\vec{u}
\label{eq:nse}
\end{align}
which describe the conservation of mass and momentum in the system. Here, $t$
denotes the time, $p^{\ast}$ the hydrodynamic pressure, and $\vec{u}$ the velocity
vector, with its components $u_{r}$, $u_{\theta}$ and $u_{z}$ pointing along the
cylindrical coordinates in the radial ($r$), azimuthal ($\theta$) and axial ($z$)
direction.
\par
For the version of \nsc with thermal convection an additional equations for the
temperature enters the stage, which couples to the momentum equation as described
in section~\ref{sec:thermalConvection}. For \nsp, the code version to compute flow
through a straight pipe geometry, the governing equations as described in detail
in section~\ref{sec:nspGoverningEquations}.

\subsection{Control parameters and non-dimensional description}
\label{sec:controlParameters}

The TC geometry, as shown in figure~\ref{fig:tc}, can be fully characterised using
only two parameters. Its relative curvature is controlled by the radii ratio
$\eta=\sfrac{r_i}{r_o}$ of the inner and outer cylinder, whereas its relative height
is controlled by $\Gamma=\sfrac{L_z}{d}$, with $d=r_{o}-r_{i}$ being the gap-width
between the inner and outer cylinder wall. With the characteristic length scale set
to $d=1$, the inner and outer radius of the domain can be calculated as
\begin{align}
r_{i} = \frac{\eta}{1-\eta}
\quad\text{and}\quad
r_{o} = \frac{1}{1-\eta}
\end{align}
for any given $\eta$. For a fixed geometry, the flow between the cylinders can then
be fully characterised by only two additional parameters, namely the Reynolds number
\begin{align}
\Reynolds = \frac{u_{\text{ref}}\cdot d}{\nu}
=\frac{2}{1+\eta}\left(\Rei-\eta\Reo\right)
\end{align}
and the rotation number
\begin{align}
\Reom = \frac{2\omega_{\text{ref}}\cdot d}{u_{\text{ref}}} =
\left(1-\eta\right) \frac{\Rei+\Reo}{\Rei-\eta\Reo}
\text{,}
\end{align}
where $u_{\text{ref}}$ is a characteristic reference velocity and
$\omega_{\text{ref}}$ a characteristic angular velocity of the co-rotating reference
frame, see \eg \cite{Dubrulle2005a} and \cite{Brauckmann2016}. The traditional Reynolds
numbers that measure the dimensionless velocity of the inner and outer cylinders in the
laboratory frame of reference, as \eg used in the seminal experiments of BrandstÃ¤ter
et al. \cite{Brandstater1983, Brandstater1987}, are given through
\begin{align}
\Rei = \frac{r_{i}\omega_{i}d}{\nu} = \frac{u_{i}d}{\nu}
\hspace{1em}\text{and}\hspace{1em}
\Reo = \frac{r_{o}\omega_{o}d}{\nu} = \frac{u_{o}d}{\nu}
\text{.}
\label{eq:reynoldsIO}
\end{align}
Henceforth, here and in the source code, all variables will be rendered dimensionless
using $d$, $\sfrac{d^2}{\nu}$, and $\sfrac{\nu^2}{d^2}$ as units for length, time,
and reduced pressure ($p=\sfrac{p^{\ast}}{\rho}$), respectively. Thus, we obtain a
non-dimensional form of the governing equations~(\ref{eq:nse}), in which \Rei and \Reo
appear only through a Dirichlet boundary condition for the velocity vector
\begin{align}
\vec{u}(r=r_{i,o}, \theta, z, t) =
\begin{bmatrix}
0 & \Reynolds_{i,o} & 0
\end{bmatrix}^{\intercal}
\label{eq:noslipBC}
\end{align}
at the inner and outer wall of the cylinders. By doing this, we generate a
impermeability and no-slip condition which models the solid wall. The unit
for the velocity is already implicitly defined through our choice for unit
length ($d$) and unit time ($\sfrac{d^2}{\nu}$), and is thus given by
$\sfrac{\nu}{d}$.
\par
For the version of \nsc with thermal convection additional parameters appear
in the equations to control the non-dimensional temperature gradient and the
fluid properties, as described in sections~\ref{sec:boundaryConditions} and
\ref{sec:thermalConvection}. For \nsp, the code version to compute flow
through a straight pipe geometry, different boundary conditions, different
driving mechanisms, and different control parameters appear in the
equations as described in detail in section~\ref{sec:nspGoverningEquations}.



\section{Building the code}
\label{sec:buildingTheCode}

\subsection{Hardware prerequisites}
\label{sec:hardwarePrerequisites}

\nsc is written in modern \fortran and, over time, has been ported to all major
CPU-based high-performance computing (HPC) platforms. Amongst IBM Power,
BlueGene and \code{x86\_64} architectures -- including a few generations
of the prevalent Intel Xeon multi-core processors -- it has also been
ported to Xeon Phi (KnightsLanding), AMD EPYC (Naples, Rome) and ARMv8.1
(Marvell ThunderX2) platforms. Optimisation for the NEC SX-Aurora vector
architecture is underway. The tutorials provided in section~\ref{sec:tutorials} of
this user guide were designed to run on standard laptops and small clusters.
\par
Additionally, we provide a GPU-accelerated (but basic) version of \nsc, which is
written in \c-\cuda and runs on a single GPU. In case you are interested in using
(or advancing) this version, you will need a \cuda-capable GPU device with compute
capability 2.0 (or superior) and support for double precision arithmetic.

\subsection{Software prerequisites}
\label{sec:softwarePrerequisites}

Building \nsc requires a Linux operating system, standard compilers and only
very few additional software libraries. All of them are commonly available as
high-quality, open source software packages or as vendor-optimised tool chains.
For the examples and tutorials (Section~\ref{sec:tutorials}) presented in this user
guide, we assume that you use a \code{bash} shell, although not necessarily
required.

\subsubsection{Compiler}
\label{sec:compiler}

You need a modern \fortran compiler which is
\href{https://en.wikipedia.org/wiki/OpenMP}{\code{OpenMP3}} compliant.
Additionally, a corresponding \c compiler is necessary. \nsc has so far been
tested with Intel's \code{ifort} and \code{icc} versions 12 through 15 as well as
\code{PSXE2017} and \code{PSXE2018}, GNU's \code{GCC-4.7} to \code{GCC-4.9} and
PGI's compilers version 14.
\par
Optionally, you will need a suitable version of NVIDIA's \cuda toolkit in case
you are interested in working with the subsidiary GPU-accelerated version of our
code. Note, that in this case a suitable choice for the \cuda version highly
depends on the hardware you are going to use. The GPU version of \nsc has so
far been tested with the \cuda toolkit version from 8.0 to 10.1. Free software
and further information can be found here:
\begin{itemize}
\item \url{https://gcc.gnu.org}
\item \url{https://developer.nvidia.com/cuda-toolkit}
\end{itemize}

\subsubsection{Message passing interface}
\label{sec:mpi}

You need an \mpi library which supports the thread-level \code{MPI\_THREAD\_SERIALIZED}.
This means that the user code is multi-threaded, but calls to the \mpi library are
serialised. A fallback option for the minimum thread-level is implemented, which is
supported by any \mpi implementation. \nsc has so far been tested with Intel's
\code{MPI-4.1}, \code{5.0}, \code{5.1} as well as \code{IBM PE 1.3} and \code{1.5}.
\par
Currently, the optional \cuda version of \nsc is not parallelised to run on multiple
nodes; it runs on a single GPU only. Free software and further information can be found
here:
\begin{itemize}
\item \url{https://www.open-mpi.org}
\item \url{https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf}
\end{itemize}

\subsubsection{Linear algebra}
\label{sec:linearAlgebra}

You need a serial \code{BLAS}/\code{LAPACK} library. \nsc has so far been tested with
Intel's \code{MKL-11.x}. The optional GPU version of \nsc relies on custom \cuda kernels
to perform linear algebra.
% -> adapt \verb+LIBLA+ and/or point environment variable \verb+$MKL_ROOT+ to a MKL installation
For free software and furthe information see:
\begin{itemize}
\item \url{https://en.wikipedia.org/wiki/LAPACK}
\item \url{http://www.openblas.net}
\item \url{http://math-atlas.sourceforge.net}
\item \url{http://www.netlib.org}
\end{itemize}

\subsubsection{Fast Fourier transform}
\label{sec:fftw}

You need a serial but fully thread-safe \code{FFTW3} installation
or equivalent. \nsc has so far been tested with \code{FFTW-3.3}
and also with the \code{FFT} implementation which comes with
Intel's math kernel libraries \code{MKL-11.1} to \code{MKL-11.3}.
Earlier versions of \code{MKL} will likely fail. The optional GPU
version of \nsc relies on the \code{cuFFT} library for computing
Fourier transforms.
%   -> point environment variable \verb+$FFTW_ROOT+ to a FFTW3 installation
%   -> alternative: select  make FFTLIB=MKL (requires MKL 11.1 or later)
For free software and further information see:
\begin{itemize}
\item \url{http://www.fftw.org}
\item \url{https://en.wikipedia.org/wiki/FFTW}
\item \url{https://en.wikipedia.org/wiki/Math_Kernel_Library}
\end{itemize}

\subsubsection{Hierarchical data format}
\label{sec:hdf5}

Optionally, you need an \mpi-parallel \hdf library installation to
enjoy flow field output in primitive variables (Section~\ref{sec:iohdf5}),
which can be readily visualised using \eg \paraview (Section~\ref{sec:paraview})
or \visit (Section~\ref{sec:visit}). \nsc has already been tested with
\code{HDF5-1.8.x} and \code{HDF5-1.10.0-patch1}. However, in case you don't need
\hdf output for now or encounter difficulties in providing a suitable \hdf
installation, you can ignore this software requirement and simply switch this
feature off (\code{HDF5IO=no}) when you compile the code
(Section~\ref{sec:compileTimeOptions}). \par
The GPU version of \nsc requires standard \hdf libraries and provides an output
compatible with the \fortran code for visualisation.
Free software and further information can
be found here:
\begin{itemize}
\item \url{http://www.hdfgroup.org/HDF5}
\item \url{https://portal.hdfgroup.org/display/support}
\item \url{https://en.wikipedia.org/wiki/Hierarchical_Data_Format}
\end{itemize}
% -> point environment variable \verb+$HDF5_ROOT+ to an MPI-parallel HDF5 installation

\subsubsection{Software modules}
\label{sec:softwareModules}

If you are planing to run \nsc on a common supercomputer at one of the national
high-performance computing centres (\eg in Germany
\href{https://www.lrz.de/english/}{LRZ},
\href{https://www.hlrs.de/home/}{HLRS},
\href{http://www.fz-juelich.de/ias/jsc/EN/Home/home_node.html}{JSC},
\href{https://www.hlrn.de/home/}{HLRN}),
all of the above mentioned software packages will most likely be readily available
as user-loadable \href{http://modules.sourceforge.net/}{environment modules}.
When the environment module for a particular application/version is loaded
(unloaded), the user environment is changed such that this application becomes
available (unavailable) on the command line without changing any environment
variables manually. As a first step, you should make yourself familiar with the
system you are going to work on and see what software is already available. The
\code{module} command is a function which can take different arguments. Some useful basic
combinations to start working with modules are listed in the following.
\begin{lstlisting}[language=bash]
feldmann@fsmcluster:~/$ module list                       # list currently loaded packages
feldmann@fsmcluster:~/$ module avail                      # overview of available packages
feldmann@fsmcluster:~/$ module avail 2>&1 | grep -i intel # search for specific things
feldmann@fsmcluster:~/$ module load moduleName            # load package, default version
feldmann@fsmcluster:~/$ module switch moduleNameV2        # change to specific version
feldmann@fsmcluster:~/$ module whatis moduleName
feldmann@fsmcluster:~/$ module help moduleName
feldmann@fsmcluster:~/$ module purge                      # unload all packages
...
feldmann@fsmcluster:~/$ which mpiifort
feldmann@fsmcluster:~/$ mpiifort --version
\end{lstlisting}

\subsubsection{Self-build libraries}
\label{sec:selfBuildLibraries}

If you are planing to run \nsc on your laptop, desktop, or local institute
cluster, you will most likely have to install some or all of the above
mentioned software packages yourself, before you can build and run \nsc.
Regarding \mpi and the required compilers, it is worth trying to install
them using your favourite software package manager (e.g. \code{apt-get},
\code{rpm} and alike).
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/$ sudo apt-get install openmpi-bin libopenmpi-dev mpi-default-dev
feldmann@darkstar:~/$ sudo apt-get install gfortran gcc
feldmann@darkstar:~/$ gfortran --version
\end{lstlisting}
For the rest, it might become necessary to download the source files and
build the libraries yourself. In appendix~\ref{app:selfBuildLibraries}, you
will find detailed examples of how to correctly configure and build some
of the required libraries (\code{FFTW3}, \code{zlib} and \hdf) for the use
with \nsc on our local \code{fsmcluster} at the
\href{https://www.zarm.uni-bremen.de/en/}{ZARM} institute. It should,
however be clear, that these examples can only serve as a rough guideline
to install all necessary software packages on your target Linux system.

\subsection{Download the source files}
\label{sec:download}

Once you have checked that you have all the compilers and libraries installed
on your target system, you can proceed downloading the \nsc source files. Log
into the machine where you want to install and run \nsc. Make sure you are in
your home directory and create the \nsc working directory.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/$ cd $HOME
feldmann@darkstar:~/$ mkdir nsCouette
feldmann@darkstar:~/$ cd nsCouette
\end{lstlisting}
This is the place where you should put the source files. This is also the place,
where we choose to put all the case directories -- one for each tutorial
(Section~\ref{sec:tutorials}) -- so that we have everything together in one place. Note,
however, that depending on the system you are working on, policy and quotas might
require you to store your simulation data (\ie the case directories) in
designated file systems other than your home partition.
\par
You either got the source files as a \code{tar} archive file from one of the
authors or you can download the source files from our publicly available
\href{https://github.com/dfeldmann/nsCoutte}{\code{github}} repository.
In the first case, copy the archive into the working directory you just created
and unpack it.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette$ tar xfvz nsCouette.tar.gz
feldmann@darkstar:~/nsCouette$ ls
nsCouette nsCouette.tar.gz
\end{lstlisting}
The latter option requires a working \code{git} installation, which is a
\href{https://git-scm.com/}{distributed version control system}. Most of
the time, \code{git} will either be readily available on your system or
it can be installed using your favourite software package manager.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette$ sudo apt-get install git
...
feldmann@darkstar:~/nsCouette$ git clone https://github.com/dfeldmann/nsCouette
feldmann@darkstar:~/nsCouette$ cd nsCouette
feldmann@darkstar:~/nsCouette/nsCouette$ git checkout nsCouette-1.0
feldmann@darkstar:~/nsCouette/nsCouette$ git checkout nsCouette-gpu
feldmann@darkstar:~/nsCouette/nsCouette$ git checkout nsPipe-1.0
feldmann@darkstar:~/nsCouette/nsCouette$ git status
feldmann@darkstar:~/nsCouette/nsCouette$ git pull
\end{lstlisting}
Once you have \code{git} running, the \nsc source file remote
repository can than easily be cloned (downloaded) to your target
system with one single command. This first step has to be done
only once and by default you should be on the branch \code{nsCouette-1.0}.
But you can easily switch to other branches using the
\code{checkout} command to have access to the pipe flow variant
or the GPU-accelerated \cuda version. The
\code{git} commands \code{status} and \code{pull} will help you
to figure out on which branch you currently are and will get you
the latest changes (\eg bug fixes or new features) from the remote
repository. A few more helpful hints on how to use \code{git} can
be found in appendix~\ref{app:git}.

\subsection{Compile the code}
\label{sec:compile}

Once you have downloaded the source files (Section~\ref{sec:download}),
you can proceed building the executable. Go to the top-level directory
of the source files. Among other things, here you will find the following
files and directories, which are important to compile \nsc.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/nsCouette$ ls
...
ARCH/     scripts/    Makefile    nsCouette.f90   perfdummy.f90
mod_fdInit.f90  mod_inOut.f90  mod_timeStep.f90  mod_fftw.f90    mod_hdf5io.f90
mod_myMpi.f90   mod_vars.f90   mod_getcpu.f90 mod_nonlinear.f90  mod_params.f90
...
\end{lstlisting}
The source code is organised in twelve \fortran files (\code{.f90}), as
described in section~\ref{sec:codeStructures} in more detail. Among other things,
the directory \code{scripts} contains example files which will help you
to easily set-up the correct environment variables which are necessary for
building -- and also running -- the code. Have a look at them and make a
copy of one of the templates which appears closest to your platform/situation
and modify it accordingly using your favourite text editor (\eg \code{vi}).
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/nsCouette/scripts$ ls
nsCouetteAtDarkstar.sh  nsCouetteAtFsm.sh  nsCouetteAtKonrad.sh  nsPipeAtKonrad.sh
submit_loadl.sh  submit_sge.sh  submit_slurm.sh
feldmann@darkstar:~/nsCouette/nsCouette/scripts$ cp n*Darkstar.sh nsCouetteAtMyPlatform.sh
feldmann@darkstar:~/nsCouette/nsCouette/scripts$ vi nsCouetteMyPlatform.sh
\end{lstlisting}
By modifying we mean, that you should load the desired modules
(Section~\ref{sec:softwareModules}) and/or set all necessary
library path variables correctly (Section~\ref{sec:selfBuildLibraries}).
Also, you might find it convenient to create an alias in your
\code{.bashrc} so that you can easily load the correct environment
by simply typing \code{nsc} any time you log into your machine
or open a new terminal window. Note, that you have to load this
environment every time you want to (re-) compile the code and
also every time you want to run the executable (Section~\ref{sec:runningTheCode}).
\begin{lstlisting}[language=bash]
feldmann@darkstar:~$ vi .bashrc
...
alias nsc="source $HOME/nsCouette/nsCouette/scripts/nsCouetteAtDarkstar.sh"
...
feldmann@darkstar:~$ source .bashrc
feldmann@darkstar:~$ nsc
\end{lstlisting}

\subsubsection{Makefile settings}
\label{sec:makefile}

In general, the \code{Makefile} itself requires no editing by the user.
It is, however, advisable to have a look at it at some point to get an
idea of how \nsc is structured and build. Instead of modifying the
\code{Makefile}, all platform-specific settings should be fixed using
the architecture files. You can find a variety of working examples for
different systems in the respective directory. Have a look, copy the
template which appears closest to your platform/situation and modify it
accordingly using your favourite text editor (\eg \code{vi}).
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/nsCouette/ARCH$ ls
make.arch.darkstar   make.arch.gcc-openblas  make.arch.Supermuc
make.arch.fsm  make.arch.Hydra   make.arch.XC30_sisu-cray
make.arch.gcc-atlas  make.arch.intel-mkl  make.arch.XC30_sisu-intel
make.arch.gcc-essl   make.arch.KNL-intel  make.arch.xl-essl
make.arch.gcc-linux  make.arch.nec-aurora
make.arch.gcc-mkl make.arch.pgi-mkl
feldmann@darkstar:~/nsCouette/nsCouette/ARCH$ cp make.arch.darkstar make.arch.myPlatform
feldmann@darkstar:~/nsCouette/nsCouette/ARCH$ vi make.arch.myPlatform
\end{lstlisting}
By modifying we mean, that you should configure the correct compiler and libraries
according to your environment and also specify the desired compiler options (\eg
hardware specific optimisation flags). In case you use self-made libraries (see
section~\ref{sec:selfBuildLibraries} and appendix~\ref{app:selfBuildLibraries}),
make sure to correctly set all necessary path variables (\eg \code{LD\_LIBRARY\_PATH}).
Please consult your local IT support in case of problems.
\par
If everything is configured correctly, the executable can be (re-)build in the usual way by
simply typing \code{make} and specifying the desired architecture file. It is recommended
to always \code{clean} the build directory before each compilation.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/nsCouette$ make ARCH=myPlatform clean
feldmann@darkstar:~/nsCouette/nsCouette$ make ARCH=myPlatform
...
feldmann@darkstar:~/nsCouette/nsCouette$ ls
ARCH/ darkstar/ myPlatform/ myPlatformGcc/ myPlatformIntel/
...
feldmann@darkstar:~/nsCouette/nsCouette$ ls darkstar/
nsCouette.x
...
\end{lstlisting}
If the code compiles correctly, a new sub directory is generated. It contains all the
generated objects (\code{.o}) and modules (\code{.mod}) as well as the executable
\code{nsCouette.x} itself. The subdirectory is named after the architecture file you
specified (here \eg \code{ARCH=myPlatform}). Specifying another architecture file, will
generate another subdirectory with another executable. This way you are able to easily
manage different builds side by side.

\subsubsection{Compile-time options}
\label{sec:compileTimeOptions}

Besides specifying the architecture file (Section~\ref{sec:makefile}),
the \code{make} command also takes a few other options which help you to
control the build process. The different options listed below can be
arbitrarily combined. They are meant to switch off \hdf flow field output
(Section~\ref{sec:iohdf5}), to include integrating the temperature for
thermal convection and heat transfer (Section~\ref{sec:thermalConvection}),
and to build the code in debug mode (\ie tightest debug settings of the
compiler and no hardware specific optimisation). The respective default
value is the first choice in the angled brackets.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/nsCouette$ make ARCH=myPlatform HDF5IO=<yes|no>
feldmann@darkstar:~/nsCouette/nsCouette$ make ARCH=myPlatform CODE=<STD_CODE|TE_CODE>
feldmann@darkstar:~/nsCouette/nsCouette$ make ARCH=myPlatform DEBUG=<no|yes>
\end{lstlisting}



\section{Running the code}
\label{sec:runningTheCode}

Once you have compiled the executable (Section~\ref{sec:compile}), you
are almost ready to run a simulation. Create a case directory and make
sure that you have at least the executable and a parameter input file
(Section~\ref{sec:parameterInputFile}) in that case directory.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/nsCouette$
feldmann@darkstar:~/nsCouette/nsCouette$ mkdir myCase01
feldmann@darkstar:~/nsCouette/nsCouette$ cp nscouette/myPlatform/nsCouette.x  myCase01/.
feldmann@darkstar:~/nsCouette/nsCouette$ cp nscouette/myPlatform/nsCouette.in myCase01/.
feldmann@darkstar:~/nsCouette/nsCouette$ cd myCase01
feldmann@darkstar:~/nsCouette/nsCouette/myCase01$ ls
nsCouette.in   nsCouette.x
feldmann@darkstar:~/nsCouette/nsCouette/myCase01$ ./nsCouette.x < nsCouette.in
\end{lstlisting}
Basically, the parameter input file is simply passed to the standard input
when you call the executable. A few detailed examples are described in the
tutorials (Section~\ref{sec:tutorials}).

\subsection{Parameter input file}
\label{sec:parameterInputFile}

In general, \nsc has to be build only once on a system and the executable
can be used for all your simulations within one research project. Most of
the settings and parameters to control the simulation can be specified at
run-time using a parameter input file. This file can have any name (here
\code{nsCouette.in}) and it is simply passed to the standard input when
calling the executable (here \code{nsCouette.x}). The parameter input
file should contain all of the following \fortran namelists. However, the
keywords within a namelist can appear in any order and individual keywords
can also be omitted if not needed.
\begin{lstlisting}[language=fortran]
&parameters_grid
m_r   = 32                   ! Radial points           => m_r      grid points
m_th  = 16                   ! Azimuthal Fourier modes => 2*m_th+1 grid points
m_z0  = 16                   ! Axial Fourier modes     => 2*m_z0+1 grid points
k_th0 = 6.0                  ! Fundamental wavenumber  => L_th = 2*pi/k_th0
k_z0  = 2.6179938779914944d0 ! Fundamental wavenumber  => L_z  = 2*pi/k_z0
eta   = 0.868d0              ! Radii aspect ratio
alpha = 0.0d0                ! Distribution of radial points, 0=Chebyshev, 1=uniform, alpha<0 read from file
/

&parameters_physics
Re_i =   200.0d0           ! Inner cylinder Reynolds number
Re_o =  -200.0d0           ! Outer cylinder Reynolds number
Gr   =    50.0d0           ! Grashof number, Gr = Ra/Pr            [TE_CODE only]
Pr   =     0.71d0          ! Prandtl number                        [TE_CODE only]
gap  =     3.25d0          ! Gap size in cm                        [TE_CODE only]
gra  =   980.0d0           ! Gravitational acceleration in g/cm**3 [TE_CODE only]
nu   =     1.01d-2         ! Kinematic viscosity in cm**2 /s       [TE_CODE only]
/

&parameters_timestep
numsteps    = 10000        ! Number of timesteps to compute
variable_dt = T            ! Use a variable (T) or a constant (F) timestep size
init_dt     = 1.00d-5      ! Initial (T) or constant (F) size of timestep
maxdt       = 1.00d-2      ! Maximum allowed size of timestep (T)
Courant     = 0.25d+0      ! CFL safety factor
/

&parameters_output
fBase_ic = 'myCase01'   ! identifier for coeff_ (checkpoint) and fields_ (hdf5) files
dn_coeff = 2000         ! output interval [steps] for coeff (dn_coeff =-1 disables output)
dn_ke    = 100          ! output interval [steps] for energy
dn_vel   = 100          ! output interval [steps] for velocity
dn_Nu    = 100          ! output interval [steps] for Nusselt (torque)
dn_hdf5  = 1000         ! output interval [steps] for HDF5 output
dn_prbs   = 10          ! output interval [steps] for time series data at probe locations
prl_r(1)  = 0.20d0      ! radial probe locations (0 < r/d < 1)
prl_r(2)  = 0.50d0
prl_r(3)  = 0.80d0
prl_r(4)  = 0.20d0
prl_r(5)  = 0.50d0
prl_r(6)  = 0.80d0
prl_th(1) = 0.25d0      ! azimuthal probe locations (0 < th/L_th < 1)
prl_th(2) = 0.25d0
prl_th(3) = 0.25d0
prl_th(4) = 0.75d0
prl_th(5) = 0.75d0
prl_th(6) = 0.75d0
prl_z(1)  = 0.25d0      ! axial probe locations (0 < z/L_z < 1)
prl_z(2)  = 0.25d0
prl_z(3)  = 0.25d0
prl_z(4)  = 0.75d0
prl_z(5)  = 0.75d0
prl_z(6)  = 0.75d0
print_time_screen = 100 ! output interval [steps] for timestep info to stdout
/

&parameters_control
restart = 0             ! from scratch (0) or restrat from checkpoint file (1,2)
runtime = 86400         ! maximum wall-clock time for the job in seconds
/

&parameters_initialcondition
ic_tcbf = T ! Set Taylor-Couette base flow (T) or resting fluid (F), only when restart = 0
ic_temp = F ! Set temperature profile (T) or zero (F), only when restart = 0, only TE_CODE
ic_pert = T ! Add perturbation on top of base flow (T) or not (F), only when restart = 0
ic_p(1, :) = 4.0d-2, 0, 1 ! 1st perturbation: amplitude and wavevector (a1, k_th1, k_z1)
ic_p(2, :) = 6.0d-3, 1, 0 ! 2nd perturbation: amplitude and wavevector (a2, k_th2, k_z2)
ic_p(3, :) = 0.0d-0, 0, 0 ! 3rd perturbation: amplitude and wavevector (a3, k_th3, k_z3)
ic_p(4, :) = 0.0d-0, 0, 0 ! Add up to six user defined perturbations
/
\end{lstlisting}

\subsubsection{Control the time stepper}
\label{sec:controlTimeStepper}

The temporal integration scheme has been upgraded to a predictor-corrector method, see
section~\ref{sec:timeStepper} and Guseva et al.~\cite{Guseva2015}. This enables a
variable timestep size ($\Delta t$), which is dynamically controlled during runtime.
This feature is of particular advantage, if the flow state is either suddenly modified
(applying disturbances, changing rotation rates etc.) or naturally undergoes strongly
transient dynamics.
\par
Most aspects of the time stepper can be easily controlled on a user-level using the
name list \code{parameters\_timestep} in the parameter input file (see
section~\ref{sec:parameterInputFile}). Here, the number of timesteps can be specified,
which determines how many times the code runs through the main time integration loop.
Additionally, you can choose between a user-specified constant $\Delta t$ and a
variable $\Delta t$. If you chose a variable timestep size by setting \code{variable\_dt=T},
then $\Delta t$ will be automatically updated every now and then according to the limits
you can also specify in this name list --- the care-free package! In case
you start a simulation from scratch (\code{restart=0}) the dynamic adaptation
process begins with the value you specified at \code{init\_dt} in the parameter
file. However, in case you restart your simulation from an existing flow field
file (\code{restart=<1|2>}), then the \code{init\_dt} in the parameter file will
be ignored and the dynamic adaptation process resumes with the last value for
$\Delta t$ which was stored within the velocity file. If you, on the other hand,
chose a constant timestep size by setting \code{variable\_dt=F}, then $\Delta t$
will always be set to the value you specify for \code{init\_dt} in the parameter
and it will not change during the simulation. Further possibilities to control the
time stepper, which however require rebuilding the code, are described in
section~\ref{sec:timeStepper}.
\par
Usually, you run the transient phase of a simulation with \code{variable\_dt=T}
and observe the development of $\Delta t$. As soon as you have reached a statistically
steady state and want to calculate statistics or create output for movies and what not,
then you resume the simulation with a constant $\Delta t$, which you set to a nice even
value (\code{init\_dt}) which fits your needs and is slightly below the minimum
$\Delta t$ of what you observed during the transient phase.

\subsubsection{Setting the radial grid}
\label{sec:radialGrid}

The radial distribution of grid points can be specified at runtime by modifying the
\code{alpha} parameter ($\alpha$) included in the \code{parameters\_grid} name list. The
nodes are distributed as
\begin{align*}
r_j = \frac{1+\eta}{2(1-\eta)} +
\frac{\operatorname{asin}\left(-\alpha\cos\left(\frac{\pi j}{m_r-1}\right)\right)}
{2\operatorname{asin}\left(\alpha\right)}
\quad\text{with}\quad
j=0,\dots, m_r-1
\text{.}
\end{align*}
Note that $\alpha$ must have a value between $0$ and $1$. For $\alpha = 1$, the nodes are
uniformly distributed, whereas for $\alpha = 0$, a Chebyshev grid with strong quadratic
clustering near the walls is obtained. Radial grids whose behaviour falls between Chebyshev
and equispaced grids are obtained if intermediate values of $\alpha$ are chosen.
\par
Alternatively, the radial grid can be read from an external file. This option is
enabled if $\alpha$ is set to a negative value. The file containing the radial grid point
coordinated must be named \code{radial\_distribution.in} and the points must be arranged
in ascending order, \ie from the inner to the outer cylinder.
\par
When the simulation is started from a checkpoint file (see section~\ref{sec:checkpoint}),
the code automatically detects if the radial grid has been modified and interpolates the
solution to the new grid. This is performed by comparing the value of $\alpha$ in the input
file (parameters of the current simulation) and the restart file (parameters of the simulation
where the checkpoint file was generated). Note that if the grid of the checkpoint file is
specified by an external file, this must be named \code{radial\_distribution\_old.in}. If
both the new and old grids are specified in files, these must be named \code{radial\_distribution.in}
and \code{radial\_distribution\_old.in}, respectively, and the value of $\alpha$ in the input
file must be different (\ie a different negative number) than that in the restart file. This
enables the code to identify that the grids are different and interpolation is required.


\subsubsection{Setting boundary conditions}
\label{sec:boundaryConditions}

The boundary conditions (BC) in the two homogeneous directions ($\theta$ and $z$) are
periodic; basically for all flow field variables. The BC at the solid cylinder walls can be
controlled through he name list \code{parameters\_physics}. To model the impermeable
but rotating cylinder walls of Taylor-Couette set-up, we impose Dirichlet boundary
conditions for the velocity field. The two cross-stream velocity components ($u_r$ and
$u_z$) are set to zero everywhere at the walls. The streamwise velocity component
($u_{\theta}$) is set to \Rei and \Reo everywhere at the inner and outer cylinder wall,
respectively, which corresponds to the rotation speed of the respective wall, see
eq.~(\ref{eq:reynoldsIO}).
\par
For the heat transfer version of \nsc, additional Dirichlet BC are imposed for the
temperature field to model the isothermal cylinder walls. The temperature difference
between the inner and outer cylinder wall can be controlled by choosing the desired
Rayleigh number (\ie setting values for the Grashof and the Prandtle number) in the
same name list.

\subsubsection{Choosing initial conditions}
\label{sec:initialConditions}

Integrating the Navier-Stokes equations forward in time is a typical initial
value problem. Each simulation run needs proper initial conditions for the
full three-dimensional velocity field $\vec{u}(r,\theta,z,t_0)$ and --
optionally -- also for the temperature field $T(r,\theta,z,t_0)$. Basically,
you have two options how to specify initial values for the flow field
variables, which can be controlled using the keyword \code{restart} in the
parameter name list \code{parameters\_control}.
\begin{itemize}
\item
You can choose to generate initial conditions in the code itself
(\code{restart=0}). Starting from scratch requires no additional
files and data other than the executable itself (\code{nsCouette.x})
and a parameter input file (\code{nsCouette.in}). You can run a
simulation right away, as exemplarily shown in the first tutorial
in section~\ref{sec:tc0040}. However, there are a few different
options how to generate initial conditions, as described below, and
some more will be implemented in the future.
\item
Or you can choose to read initial conditions from an existing file
you have at hand (\code{restart=<1|2>}). This file has to be a Fourier
coefficient file in binary format (Section~\ref{sec:ioCoeff}), which
you might have from a similar simulation case or from a former run
of the same case. See section~\ref{sec:checkpoint} for details of
the checkpoint-restart mechanism.
\end{itemize}


You can choose to prescribe a resting fluid as initial condition by
setting \code{ic\_tcbf=F}. To prescribe the (Taylor)-Couette
analytical solution as base flow set \code{ic\_tcbf=T}. No matter
what the base flow is, you can disturb it by superimposing up to
six different perturbations by setting \code{ic\_pert=T} and than
defining an amplitude and wave number vector for each perturbation.
The perturbations are designed to fulfil the no slip boundary
condition at the solid walls and to be divergence free by construction.
Figure \ref{fig:ic} shows some useful examples of different initial
conditions at $\Rei=\num{50}$.
\begin{figure}
\begin{subfigure}{1.0\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{figures/tc0039/icTaylorCouette.png}
\caption{Taylor-Couette analytical solution, no perturbation.}
\label{fig:tc0039icTayolorCouette}
\end{subfigure}
\vfill
\begin{subfigure}{1.0\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{figures/tc0039/icPerturb01.png}
\caption{Resting fluid, perturbed in mode $(l_{\theta,1}, n_{z,1})=(0,1)$
with amplitude $a_{1}=\num{e2}$.}
\label{fig:tc0039icPerturb01}
\end{subfigure}
\vfill
\begin{subfigure}{1.0\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{figures/tc0039/icPerturb10.png}
\caption{Resting fluid, perturbed in mode $(l_{\theta,1}, n_{z,1})=(1,0)$
with amplitude $a_{1}=\num{e2}$.}
\label{fig:tc0039icPerturb10}
\end{subfigure}
\vfill
\begin{subfigure}{1.0\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{figures/tc0039/icPerturb01and10.png}
\caption{Resting fluid, perturbed in modes $(0,1)$ and  $(1,0)$
with amplitudes $a_{1}=a_{2}=\num{e2}$.}
\label{fig:tc0039icPerturb01and10}
\end{subfigure}
\vfill
\begin{subfigure}{1.0\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{figures/tc0039/icPerturb22.png}
\caption{Taylor-Couette base flow, perturbed in mode
$(l_{\theta,1}, n_{z,1})=(1,1)$ with amplitude $a_{1}=\num{e2}$.}
\label{fig:tc0039icPerturb22}
\end{subfigure}
\caption{Useful examples of different initial conditions at $\Rei=\num{50}$.
Shown are iso-contours (red/blue: $u_{\alpha}=\sfrac{\pm\Rei}{10}$) for all
three velocity components.}
\label{fig:ic}
\end{figure}
% Work in progress: Soon it will also be possible to read initial conditions from
% a \code{coeff*} file and add a perturbation on top by combining \code{restart=1}
% and \code{ic\_pert=T}.

\subsection{On your laptop/desktop}
\label{sec:onYourLaptopDesktop}

On your local machine you simple need to open a terminal and go to one of your case
directories. If you do not have one, create one (Section~\ref{sec:runningTheCode}).
Here, we use the first tutorial case~\ref{sec:tc0040} as an example. Copy the executable
and start the simulation with only one \mpi process and only one \omp thread by simply
typing:
\begin{lstlisting}[language=bash]
feldmann@darkstar:~$ cd nsCouette/tc0040
feldmann@darkstar:~/nsCouette/tc0040$ cp ../nscouette/darkstar/nsCouette.x .
feldmann@darkstar:~/nsCouette/tc0040$ export OMP_NUM_THREADS=1
feldmann@darkstar:~/nsCouette/tc0040$ ./nsCouette.x < nsCouette.in
\end{lstlisting}
This starts a completely serial run without no parallelisation at all. To run \nsc with
more than one \mpi task, here \eg four, type
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0040$ export OMP_NUM_THREADS=1
feldmann@darkstar:~/nsCouette/tc0040$ mpiexec -np 4 ./nsCouette.x < nsCouette.in
\end{lstlisting}
This starts a parallel simulation purely based on \mpi without any \omp parallelisation
involved. For making use of the hybrid \mpi-\omp parallelisation strategy, the global
variable \code{OMP\_NUM\_THREADS} has to be set to a positive value other than unity. See
section~\ref{sec:controlHybridScheme} for a detailed description of how to control the
hybrid scheme.
\par
Note, that the number of \mpi tasks can be freely selected at runtime with the only
restriction that it must evenly divide the number of radial grid points
($N_{r}\equiv\code{m\_r}$), which are selected in the \code{nsCouette.in} input file, see
Section~\ref{sec:parameterInputFile}. There is no such restriction regarding the total
number of Fourier modes (\code{m\_f}), see section~\ref{sec:spatialDiscretisation}
and appendix~\ref{sec:listOfParameters}). In these cases, where the number of chosen \mpi
tasks does not evenly divide \code{m\_f}, a few dummy modes are automatically added. This
causes a small imbalance and waste of computational resources, which is reported at the
beginning of the run, but gives enormous flexibility in choosing spatial discretisation
and degree of parallelisation.

\subsection{On a cluster}
\label{sec:onCluster}

Example batch submission scripts for the Sun Grid Engine (SGE) and
SLURM with Intel MPI, and IBM LoadLeveller with IBM MPI are provided
in the subdirectory \code{scripts}. Note that these scripts can serve only
as a rough guideline. They worked on a number of HPC systems but most
probably require some adaptation to a specific batch environment.
\par
In this example, a number of \code{numsteps=10000} timesteps would be attempted
to run, but the code would stop after a maximum \code{runtime=86400} seconds
(\SI{24}{\hour}) of elapsed wall-clock time depending on what limit is reached
first. We have implemented an intrinsic safety margin corresponding to the
average number of seconds it takes to perform two timesteps.
\par
If \omp is enabled -- this is the default behaviour when compiling the code --
environmental variables must be used to control the number of threads you would
like to use and other related stuff. For example, specify
\begin{lstlisting}[language=bash]
export OMP_NUM_THREADS=4
export OMP_PLACES=cores
export OMP_SCHEDULE=static
export OMP_STACKSIZE=256M
\end{lstlisting}
if you want to use four \omp threads. The second line maps the threads to
(physical) cores. The fourth line sets the stack size; experiment with larger or
smaller values if unexpected segmentation faults (\code{SEGFAULT}) occur or if the
memory per core is scarce, respectively. These are just basic usage guidelines for
\mpi and \omp. For an advanced use of \mpi and \omp (\eg using \code{bash}
scripts) or in case of problems, please consult your local IT support.

\subsection{Control the hybrid parallelisation scheme}
\label{sec:controlHybridScheme}

The hybrid parallelisation scheme of \nsc relaxes the limit imposed by the one-dimensional (1d)
slab decomposition on the maximum number of processor cores and maps naturally to the
prevailing multi-node, multi-core HPC architectures (See
section~\ref{sec:parallelisationScheme}). Specifically, the flexibility to chose an appropriate
combination for the number of \mpi tasks per node (\tpn) and the number of \omp threads per
\mpi task (\tpt) over time has proven key for achieving good performance across a rather wide
range of node architectures. Figure~\ref{fig:runtime} illustrates the performance of \nsc
on different architectures ranging from \num{20} to \num{64} physical cores per compute node.
\begin{figure}[htb]
\centering
\includegraphics[width=1.00\columnwidth]{figures/nscouetteBenchmark.pdf}
\caption{Runtime per timestep and breakdown into the main algorithmic
components (different colours) of a typical \nsc run ($N_r=\num{512}$ and
$\num{513}\times\num{1025}$ Fourier modes) computed on \num{32} and \num{64}
dual-socket nodes of various HPC clusters, using a platform-specific number
of \mpi tasks/node (TpN).
IVB: Intel Xeon E5-2680v2 (IvyBridge), \num{20} cores/node.
BDW: Intel Xeon E5-2698v4 (Broadwell), \num{40} cores/node.
SKL: Intel Xeon 6148 (Skylake), \num{40} cores/node.
KNL: Xeon Phi 7230 (KnightsLanding), \num{64} cores/node.
ARM: Marvell ThunderX2 ARM v8.1, \num{64} cores/node.
The IVB and BDW clusters employ a Mellanox InfiniBand FDR network
(\SI{56}{\giga\bit\per\second}), whereas SKL and KNL use Intel OmniPath
(\SI{100}{\giga\bit\per\second}). The ARM cluster is interconnected
with Cray Aries (\SI{80}{\giga\bit\per\second}). \nsc was built using
platform-optimised software tool chains (\ie compilers and libraries)
but no platform-specific Optimisation of the source code was performed.
Corresponding architecture files (\code{nsCouette/ARCH}) are shipped with
the code (Section~\ref{sec:makefile}).}
\label{fig:runtime}
\end{figure}
Note, that \tpn is typically something like four or eight and the product ($\tpn\times\tpt$) is
supposed to be equal the total number of physical cores per node. The most critical performance
issue when running the code in hybrid mode (\ie with \omp enabled), can be an improper pinning
of the \mpi tasks and \omp threads to the CPU cores. We suggest the basic settings above based
on the \code{OMP\_PLACES} environment variable from the
\omp standard. However, different compilers and node architectures (NUMA, \hyperthreading,
\dots) might require specific settings. The actual mapping can be checked by examining the
standard output of \nsc. Consider the following example. You are working on a machine with
\num{16} dual-socket compute nodes. Each compute node has two CPUs with \num{12} physical
cores each. So there are \num{24} physical cores per node and \num{384} physical cores in
total. Then it would be a good first try to run \nsc with \num{32} \mpi tasks -- \ie two tasks
per node and one task per CPU, respectively -- and twelve \omp threads per \mpi task and CPU,
respectively. For this example, the standard output of \nsc should look something like this:
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0040$ export OMP_NUM_THREADS=12
feldmann@darkstar:~/nsCouette/tc0040$ mpiexec -np 32 ./nsCouette.x < nsCouette.in
...
TASKS: 32 THREADS:12 THIS: 0 Host:nid01226 Cores:  0  1  2  3  4  5  6  7  8  9 10 11
TASKS: 32 THREADS:12 THIS: 1 Host:nid01226 Cores: 12 13 14 15 16 17 18 19 20 21 22 23
TASKS: 32 THREADS:12 THIS: 2 Host:nid01227 Cores:  0  1  2  3  4  5  6  7  8  9 10 11
TASKS: 32 THREADS:12 THIS: 3 Host:nid01227 Cores: 12 13 14 15 16 17 18 19 20 21 22 23
TASKS: 32 THREADS:12 THIS: 4 Host:nid01228 Cores:  0  1  2  3  4  5  6  7  8  9 10 11
TASKS: 32 THREADS:12 THIS: 5 Host:nid01228 Cores: 12 13 14 15 16 17 18 19 20 21 22 23
...
\end{lstlisting}
It is important to note, that the individual lines can appear in random order and, that the
numbering scheme for the cores is due to some low-level settings in the operating system and
hence is highly machine specific. Tools like \code{cpuinfo} (comes with Intel \mpi) or
\href{http://code.google.com/p/likwid}{\code{likwid-topology}} can be used to check the
numbering scheme of a specific computer.
\par
It is recommended to map at least one \mpi task to each NUMA domain (CPU socket) and specify
\code{OMP\_NUM\_THREADS} to the number of physical cores per node divided by the number of \mpi
tasks you want map to each compute node. \hyperthreading is not beneficial for
\nsc. On contemporary HPC clusters with two Intel or AMD CPUs per node, we typically use two
\mpi tasks per compute node and set:
\begin{lstlisting}[language=bash]
export OMP_NUM_THREADS=8  # 1 node = 2 x ( 8-core CPU, Intel Xeon E5-2670, SandyBridge)
export OMP_NUM_THREADS=16 # 1 node = 2 x (16-core CPU, Intel Xeon E5-2698v3, Haswell)
export OMP_NUM_THREADS=20 # 1 node = 2 x (20-core CPU, Intel Xeon Gold-6148, Skylake)
...
\end{lstlisting}
Note, however, that in the part of the code, where the radial derivatives of the velocities are
Fourier-transformed to compute the nonlinear terms, the \omp-parallelism is limited by
\code{3*mp\_r}. In case of thermal convection (Section~\ref{sec:thermalConvection}), it is
limited by \code{4*mp\_r}, because of the additional array for the temperature field. Recall,
that \code{mp\_r} is the number of radial grid points per \mpi task. For the interested reader,
this part of the code can be identified by the performance \code{fft} in the
\code{mod\_nonlinear.f90} source file. Thus, if the maximum number of \mpi tasks is chosen for a
given number of radial points (and hence \code{mp\_r=1}), it is advisable to use four or eight
\mpi tasks per node on large multi-core CPUs, and decrease \code{OMP\_NUM\_THREADS}
accordingly, in order not to waste resources. A forthcoming release aims at alleviating this
efficiency bottleneck by supporting threaded FFTs.


\subsection{Output}
\label{sec:output}

\subsubsection{Time series data}
\label{sec:timeSeries}

If the program runs correctly, it immediately generates a set of data
files, which contain time series data of different quantities. These
files are updated continuously during the entire simulation run and
the sampling frequency for the time series output can be independently
specified using the keywords \code{dn\_ke}, \code{dn\_prbs} etc. in the
parameter input file, as detailed in section~\ref{sec:parameterInputFile}.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0040$ ls
ke_mode  ke_total  probe01.dat  probe03.dat  probe05.dat  torque
ke_th    ke_z      probe02.dat  probe04.dat  probe06.dat  Nusselt
...
\end{lstlisting}
The first data column always contains the physical time in viscous units
($\sfrac{d^2}{\nu}$). See section~\ref{sec:controlParameters} for more
details on non-dimensionalisation. The other columns are the relevant
physical quantities like modal kinetic energy, integral torque, as well
as pressure, velocity and temperature at particular probe locations in the
flow field. The probe locations can be specified using the respective
keywords in the namelist \code{parameters\_output}, as detailed in
section~\ref{sec:parameterInputFile}. Since these files are simple
text files, the containing time series data can be easily read, plotted and
processed using any common editor, command line tool and plotting
programme (\eg \gnuplot or \code{xmgrace}). The tutorials in
section~\ref{sec:tutorials} provide further details and examples of how
to visualise and interpret the respective time series.

\subsubsection{Fourier space flow field coefficients}
\label{sec:ioCoeff}

As described in detail in section~\ref{sec:spatialDiscretisation}, \nsc is based
on a spectral discretisation using a Fourier-Galerkin expansion in two of the
three spatial directions. Therefore, the entire information about an instantaneous
flow field snapshot is stored and treated in terms of spectral coefficients.
Input and output of the spectral coefficients is documented here in this section,
whereas optional output in primitive variables ($\vec{u}, p, T$) is discussed in
section~\ref{sec:iohdf5}.
\par
The spectral coefficients are dumped to individual files (\code{coeff\_}) for each time
step at a user-specified output interval. For a negative interval (\code{dn\_coeff=-1}),
continuous output is disabled. However, in any case, one coefficient file is written
after the very last timestep of a correctly terminated simulation run. The coefficient
files, also called checkpoint files, are always written in binary \code{MPI-IO} format.
They are meant to be read by \nsc itself or by derived post-processing tools using the
parallel I/O infrastructure of \nsc. A small human-readable text file (\code{coeff\_*.info})
containing all relevant metadata is written automatically with each coefficient file.
\par
The binary coefficient files will be generated every now and then, according to the
setting of the \code{dn\_coeff} parameter in the input file (\code{nsCouette.in}),
see section~\ref{sec:parameterInputFile}. For example, if you specify to output a
coefficient file every \num{2000} timesteps and run the simulation in total for
\num{11000} timesteps starting from scratch at timestep zero, in the end there
will be six coefficient files in total.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/myCase01$ ls coeff*
coeff_myCase01.00002000     coeff_myCase01.00002000.info
coeff_myCase01.00004000     coeff_myCase01.00004000.info
coeff_myCase01.00006000     coeff_myCase01.00006000.info
coeff_myCase01.00008000     coeff_myCase01.00008000.info
coeff_myCase01.00010000     coeff_myCase01.00010000.info
coeff_myCase01.00011000     coeff_myCase01.00011000.info
\end{lstlisting}
These files contain the Fourier coefficients representing the velocity field
and optionally the temperature field at one particular timestep; also called
snapshots. Based on these coefficient files, \nsc implements an easy-to-use
checkpoint-restart mechanism for handling long-running simulations. How the
\code{coeff} files are used as a checkpoint to continue a simulation and how
the restart mechanism is controlled, is detailed in section~\ref{sec:checkpoint}.
\par
The coefficient files can also easily be used for post-processing with tools
the user has to implement or derive from main code himself. One example of such
a post-processing tool is provided in our repository.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/nsCouette/postproc$ ls *.f90
mod_preAnalysis.f90
waveSpeed.f90
\end{lstlisting}
Reading the \code{coeff} files with external tools (\eg \python or \plplot) is
of course possible but, however, not straightforward and has to be implemented
by the user himself. To enable easy and straightforward visualisations of the
flow field, \nsc also implements optional \hdf output, which is described in the
following section.

\subsubsection{Physical space flow field data}
\label{sec:iohdf5}

The primitive variables -- namely the velocity ($u_r$, $u_{\theta}$, $u_{z}$),
the pressure ($p$) and optionally the temperature ($T$) -- can be written in
\hdf format to individual files for each timestep at a user-specified output
interval. The additional \hdf output is optional and completely independent of
the output of spectral coefficients (Section~\ref{sec:ioCoeff}). All relevant
metadata for each flow field snapshot are written to small \xmf files in order
to facilitate easy analysis with common visualisation tools like \paraview and
\visit.
\par
The \hdf functionality is switched on by default and can be disabled at compile
time (\code{make HDF5IO=<yes|no>}), as detail in section~\ref{sec:compileTimeOptions}.
In that case, only spectral snapshots in binary format (Section~\ref{sec:ioCoeff})
will be written. However, if the \hdf functionality is enabled, \nsc will generate
\code{fields*.h5} files every now and then, according to the setting of the
\code{dn\_hdf5} parameter in the input file (\code{nsCouette.in}), see
section~\ref{sec:parameterInputFile}. For example, if you specify to output snapshots
every \num{2000} timesteps and run the simulation in total for \num{11000} timesteps
starting from scratch at timestep zero, in the end there will be five physical flow
field files in total.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/myCase01$ ls fields*
fields_myCase01_00002000.h5       fields_myCase01_00002000.xmf
fields_myCase01_00004000.h5       fields_myCase01_00004000.xmf
fields_myCase01_00006000.h5       fields_myCase01_00006000.xmf
fields_myCase01_00008000.h5       fields_myCase01_00008000.xmf
fields_myCase01_00010000.h5       fields_myCase01_00010000.xmf
\end{lstlisting}
The \hdf format can be readily read, manipulated and visualised with a lot of different
software tools (\eg \python, \plplot and many more). Here, we discuss and shortly
introduce two common open-source solutions; \ie \paraview (Section~\ref{sec:paraview})
and \visit (Section~\ref{sec:visit}). Both tools allow loading sequences of \xmf files
produced by \nsc, which enables the user to interactively perform comprehensive visual
and quantitative analysis of the flow field and to easily generate videos and animations.
Sample scripts based on the \python interface of \visit, as well as a custom-made \paraview
filter for handling the cylindrical coordinate system are distributed with \nsc. \visit
has a built-in operator for handling cylindrical coordinates. A detailed visualisation
tutorial is included in section~\ref{sec:}.

\subsection{Checkpoint-restart mechanism}
\label{sec:checkpoint}

On large HPC systems, the runtime of a single batch job is usually limited to something like
\SI{12}{\hour} or \SI{24}{\hour}. In order to enable long-running simulations, \nsc implements
a semi-automatic checkpoint-restart mechanism, where the initial conditions for the next run
will be read from a flow field snapshot, which was written at the end of the preceding run.
Regardless of what you specify for \code{dn\_coeff} in the parameter input file
(Section~\ref{sec:parameterInputFile}), a Fourier coefficient file (Section~\ref{sec:ioCoeff})
is written as a checkpoint at the very end of any correctly terminated simulation run.
Additionally, a small text file named \code{restart} is created, which is required to
automatically continue the simulation in a next run. It contains the same information as the
corresponding \code{*.info} file.
\par
By setting \code{restart=1} in the parameter input file (Section~\ref{sec:parameterInputFile})
the simulation takes the information from the \code{restart} file and uses the specified
checkpoint file as the new initial condition, rather than starting a new run from scratch,
which corresponds the default behaviour (\code{restart=0}). The initial time for the new
simulation run is that of the initial condition and the generated time series data
(Section~\ref{sec:timeSeries}) is appended to the existing output files.
\par
Hence, the checkpoint-restart mechanism of \nsc allows submitting a chain of (many) individual
jobs to a batch system at once and thus facilitates handling of long-running simulations. Basic
functionalities of the batch system (\eg options like \code{-hold\_jid} for \sge or
\code{--dependency} for \slurm) can be used to express the chain-type dependency of the
individual jobs.
\par
It is important to note, that the \code{restart} file is only written if a run
has finished successfully. If, on the contrary, the run terminates prematurely
(\eg due to a hardware issue, or a numerical or code-specific problem), the
last valid coefficient file can serve as a checkpoint for restarting the run. In
such a case, the \code{restart} file has to be created manually, simply by
copying the corresponding \code{coeff\_*.info} file.
\par
A third initialisation option (\code{restart=2}) is also available, which allows
to start the simulation from a checkpoint file, but resetting the physical time
and timestep counter to zero and creating new/fresh output files for the time
series data. This might be useful if you want to change, for example, resolution
within one simulation case or if you want to create a new simulation case with
different control parameters based on initial conditions you created in a former
simulation case.
\par
In any case the grid size of the continued simulation can be freely modified by
adapting the corresponding parameters in the \code{nsCouette.in} file, but the
number of radial grid points ($N_r$, \code{m\_r}) must, of course, remain to be
divisible by the number of processors (See also section~\ref{sec:parallelisationScheme}).



\section{Post-processing}

\subsection{Visualisation with \texttt{\textbf{ParaView}}}
\label{sec:paraview}

To visualise and analyse instantaneous flow field data with the open source software
tool \paraview (see figure~\ref{fig:paraViewFlowField}) just go through the introductory
steps below and follow our tutorials in section~\ref{sec:tutorials}.
\begin{figure}[htb]
\includegraphics[width=1.00\linewidth]{figures/paraViewFlowField.png}
\caption{Screenshot of the \paraview GUI and the first basic steps to
visualise and analyse the \hdf flow field output from \nsc. Follow our
tutorials in section~\ref{sec:tutorials} for further details.}
\label{fig:paraViewFlowField}
\end{figure}
The tutorials also provide a few ready-to-use state files (\code{*.pvsm})
to start with. Note, however, that this is just a collection of very
basic quick-start recipes. For more information, please consult the
\href{https://www.paraview.org/Wiki/The_ParaView_Tutorial}{official documentation}
\cite{Moreland2018} and the pertinent online discussion forums.

\paragraph{Preparation}
\begin{itemize}
\item First of all, download a recent pre-compiled version from the official
\href{www.paraview.org}{web page} and follow the instructions to install and
run it. Or download the source files and build it yourself.
\item Once you have \paraview running, you need to load a custom-made \python-filter,
which comes with \nsc. This filter can be used to perform a coordinate transformation
for every state file you load into \paraview. This is a crucial step for correct
three-dimensional rendering of the flow field, since \paraview only knows Cartesian
coordinates ($x, y, z$), whereas the flow field data in the \hdf output files is
defined in a cylindrical coordinate system ($r, z, \theta$), see section~\ref{sec:iohdf5}.
To add the custom filter to the list of known filters do the following:
\begin{enumerate}
\item Launch \paraview
\item Chose from the menu \code{Tools} $\to$ \code{Manage custom filters}
\item Press \code{Import} in the upcoming window
\item Browse to \code{\$HOME/nsCouette/nsCouette/visualisation}
\item Select the file \code{swapTransformCylCartCoord.cpd}
\item Press \code{close}
\end{enumerate}
\end{itemize}
Now the filter is known to \paraview and can be used any time you open this
installation. To perform a coordinate transformation choose
\code{Filters $\to$ Alphabetical $\to$ swapTransformCylCartCoord} from the menu.

\paragraph{Loading flow field data}

\begin{itemize}
\item
Once \paraview is installed and the transformation filter has been successfully
added, you can begin loading flow field data. You should go to the case
directory where you have stored the \hdf output files (here one of the first
tutorial cases section~\ref{sec:tc0042})
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/$ cd nsCouette/tc0042
feldmann@darkstar:~/nsCouette/tc0042$ paraview &
\end{lstlisting}
and start \paraview.
\item
Once the \paraview main window has appeared (something like in
fig.~\ref{fig:paraViewFlowField}), chose from the menu \code{File $\to$ Open}
or click on the yellow folder icon in the top left corner to open the file
selection dialogue.
\begin{figure}[htb]
\centering
\includegraphics[height=0.42\linewidth]{figures/paraViewLoadXmf.png}\hfill
\includegraphics[height=0.42\linewidth]{figures/paraViewMultipleReaders.png}
\caption{Screenshot of the file open dialogue window (left) where you
can select the  \xmf files, which act as a reader or interface
through which  \paraview access the flow field data contained in
the \hdf files. If there is more than one  \xmf reader, chose the
first on in the appearing dialogue window (right).}
\label{fig:paraViewFlowFieldLoadXmf}
\end{figure}
There you will see a list of the available \hdf files and the corresponding \xmf
files as shown in figure~\ref{fig:paraViewFlowFieldLoadXmf}. The \xmf files are
simple text files, which contain meta data about the content of the \hdf container,
see also section~\ref{sec:iohdf5}. The \xmf files act as a reader or interface
through which \paraview access the data in the \hdf container. Select one single or
a group of several \xmf files and press \code{OK}. Note, that all files with the
same base name are automatically grouped. So if there are many flow field files in
this directory, you can easily load the entire time series of snapshots by selecting
the group instead of a single \xmf file. Note, however, that loading a large number
of snapshots might take very long for large simulations!
\item
If a new dialogue window appears (fig.~\ref{fig:paraViewFlowFieldLoadXmf}
right) after selecting snapshots and pressing \code{OK}, then select the
option \code{Xdmf3 Reader}; but not \code{Xdmf3 Reader (Top Level Partition)}
or anything else. Otherwise ignore this step.
\item
Now press \code{Apply} in the main \paraview window to actually load the selected snapshots.
Note, that, since \paraview is designed to deal with large data sets, no action or data
manipulation takes actually place until you finally hit the \code{Apply} button.
\item
Note, that data extraction, manipulation and plotting in \paraview is accomplished
through so-called filters. You find a complete list of all available filters by
choosing \code{Filters $\to$ Alphabetical} from the menu. The pipe line browser
on the left shows all loaded data sets and all filters applied to it as individual
objects or instants in an hierarchical order. Always make sure that you select/highlight
the correct instant to which you want to apply the next filter operation.
\item
Once the data set has been loaded, you should first apply the following
two filters in exactly this order:
\begin{enumerate}
\item
Select \code{Filters $\to$ Alphabetical $\to$ Clean to Grid} from the menu
and than hit the \code{Apply} button.
\item
Select \code{Filters $\to$ Alphabetical $\to$ swapTransformCylCartCoord} from
the menu and than hit the \code{Apply} button to perform the transformation
from cylindrical to Cartesian coordinates.
\end{enumerate}
\item Now recenter the view by clicking the \code{Zoom to data} button to
obtain something similar to what is shown figure~\ref{fig:paraViewFlowField}.
\end{itemize}

\paragraph{Visualise flow field data}
\begin{itemize}
\item
Choose the quantity you are interested in. By default, this
might be the pressure $p$. To get something similar as shown
in figure~\ref{fig:paraViewFlowField}, select the \eg the
axial velocity component $u_z$.
\item
If you have loaded more than one snapshot, you can simply hit
the green \code{Play} button to show a movie representation of
how the flow field changes with time. Note, that this process
might be very slow in case of large simulations.
\item
To extract iso-surfaces for different velocity components, as
shown for example in figure~\ref{fig:tc0041vectorPlot}, select
the instant named \code{swapTransformCylCartCoord} in the pipe
line browser, choose \code{Filters $\to$ Alphabetical $\to$
Contour} from the menu and hit \code{Apply}. Select the velocity
component you are interested in and modify the the contour levels
in the properties panel directly below the pipe line browser.
\item
To create a vector representation of the cross-stream velocity
components as for example in figure~\ref{fig:tc0041vectorPlot},
use the \code{Glyph} filter as exemplarily shown in the \paraview
state file \code{vectorPlot.pvsm} which comes with the second
tutorial described in section~\ref{sec:tc0041}.
\end{itemize}

\subsection{Visualisation using \visit}
\label{sec:visit}

As an alternative to \paraview (section~\ref{sec:paraview}), you can also use
the open source software \visit (fig.~\ref{fig:visit}), to visualise and
analyse the instantaneous flow field snapshots generated with \nsc.
\begin{figure}[htb]
\includegraphics[width=1.00\linewidth]{figures/visit.png}
\caption{A screenshot of the \code{GUI} of \visit showing a basic example
of a pseudo-colour representation of the instantaneous pressure field in
a small computational domain similar to the set-up used in the first
tutorial in section~\ref{sec:tc0040}.}
\label{fig:visit}
\end{figure}
The following basic steps will give you a good starting point to learn
how to read the \hdf files and how to render the containing flow field
data.
\begin{itemize}
\item Download and install the software from the \href{https://wci.llnl.gov/simulation/computer-codes/visit}{official webpage}.
\item Move all the \code{.h5} and \code{.xmf} files to one directory (\code{DIR\_HDF5}) of your choice.
\item Go there and start \visit. A GUI window will pop up to select/open files.
\item Enter the path to \code{DIR\_HDF5}.
\item Select the \code{.xmf} files and press the \code{OK} button.
\item Two GUI windows will appear: The \visit main window (control panel) and the Vis window (white blank).
\item Press the \code{Add} button in the middle of the main window. From the
pull-down menu select the type of you want to plot, \textit{\eg}, ``contour'' or ``pseudocolor'';
  \item  \textbf{Important step:} Add two operators one after another on the plot by clicking on the ``Operators'' next to the ``Add'' button.
 Select in the pulldown menu ``CoordinateSwap'' to change our file coordinates $(\theta,z,r)$ to $(r,\theta,z)$
 and ``Transform'' to change our coordinate system from
 cylindrical to cartesian (cf. Fig.~\ref{fig:VisIt});
  \item  Click on ``OpAtts'' on the top toolbars in the Main window and change the operator attributes according to the above specifications.
  \item  Click on ``Draw'' and you have the plot in the right Vis window (for large file, it takes time!);
  \item  Play with the plot and carefully observe the change after each operation;
\end{itemize}



\section{About the code}
\label{sec:aboutTheCode}

This section provides further documentation of \nsc; partly on a programming
level and also from a software engineering point of view. The functionalities
are summarised and the code structure, data types, variables, parallelisation
strategy etc. are outlined. This part of the user guide is meant to be
complementary to the information given our technical papers \cite{}. Additionally,
some details on selected methodological aspects are outlined. Further
technical documentation of the source code -- auto-generated with the
\code{FORD} tool -- can be found
\href{http://mjr.pages.mpcdf.de/nscouette/ford-doc/}{here}.

% \subsection{Functionalities of \nsc}

\subsection{Structure of the source code}
\label{sec:codeStructures}

The source code comprises only a few thousand lines of code, which are structured in
a minimum set of roughly a dozen basic source files -- including a \code{Makefile}.
The different parts of the code are more or less thematically structured into
separate files and \fortran modules according to the following list. Interested
developers can rely on a comprehensive, auto-generated, interactively browsable
online \href{http://mjr.pages.mpcdf.de/nscouette/ford-doc/}{\code{FORD} documentation},
which includes dependency and call graphs.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/nsCouette$ ls
...
Makefile          # Script file to compile the code
nsCouette.f90     # Main routine: Initialisation, time-stepping, finalisation
mod_params.f90    # All parameters and constants
mod_vars.f90      # Definition of all (derived) variables
mod_myMpi.f90     # MPI-related subroutines: windows, derived data types, etc.
mod_fftw.f90      # Subroutines related to fast Fourier transforms
mod_fdInit.f90    # Finite differences matrices for 1st and 2nd radial derivatives
mod_nonlinear.f90 # Subroutine to compute the nonlinear advection term (pseudo-spectral)
mod_timeStep.f90  # The predictor-corrector time-stepper
mod_inOut.f90     # Messy collection of input, output, base flow, initial conditions etc.
mod_hdf5io.f90    # HDF5 output of primitive flow field variables
mod_getcpu.f90    # Mapping of the CPU. This module cannot be included.
...
\end{lstlisting}



\subsection{Programme flow chart}
\label{sec:programmeFlowChart}

Basically speaking, the programme flow of \nsc is as follows.
\begin{enumerate}
\item Initialisation phase: \mpi, read parameter file (section~\ref{sec:parameterInputFile}), set initial conditions (section~\ref{sec:initialConditions})
\item Time-stepping: linear sub-step computation and nonlinear pseudo-computation)
\item Finalising phase: \mpi, checkpoint output (section~\ref{sec:})
\end{enumerate}

\subsection{Constants, variables \& data types}
\label{sec:constantVariablesAndDataTypes}

All (numerical) constants used in \nsc are specified in \code{mod\_params.f90}, while
all variables are given in \code{mod\_vars.f90}. The data types used for the numerical
constants and variables are mostly specified with \code{kind=4} or \code{kind=8},
indicating single or double precision, respectively. The derived data types in
\code{mod\_vars.f90} are called
\begin{itemize}
\item \code{phys} for variables in physical space
\item \code{spec} for variables in Fourier (spectral) space
\item \code{vec\_mpi} for vector variables in each \mpi sub-domain
\end{itemize}
The types \code{phys} and \code{spec} contain all physically relevant quantities, such as
the velocity vector ($\vec{u}$), the pressure ($p$), and, optionally, the temperature ($T$)
field. Arrays of type \code{vec\_mpi} are mainly used in the node-independent calculation
and the data is arranged such that \code{mp\_f} is a map of the couple
$(\code{m\_th}, \code{m\_z}/\code{np})$. The distribution of the whole array
$(\code{m\_th}, \code{m\_z})$ into each \mpi task is according to figure 2 in our CAF
paper~\cite{Shi2015}. See also section~\ref{sec:parallelisationScheme} for more details on
the parallelisation strategy.

\subsection{Spatial discretisation}
\label{sec:spatialDiscretisation}

Due to the cylindrical nature of the problem, the governing equations (see
section~\ref{sec:governingEquations}) are handled in a polar co-ordinate
system ($r$, $\theta$, z), as sketched in figure~\ref{fig:tc}. The spatial
discretisation of the flow variables is obtained using finite differences
(FD) in the wall-normal or radial direction $r$ and using a Fourier--Galerkin
ansatz in the two homogeneous directions $\theta$ and $z$. The FD
discretisation in $r$ is discussed in more detail in section~\ref{sec:radialGrid}.
The Fourier--Galerkin expansion in the azimuthal and axial direction for
the velocity vector ($\vec{u}$), the pressure ($p$) and, optionally, the
temperature ($T$) is given by
\begin{align}
\vec{u}(r,\theta,z) & =
\sum_{l=-L}^{L}\sum_{n=-N}^{N}\hat{\vec{u}}(r,n,l)\cdot e^{i\left(n k_{\theta}\theta+l k_{z} z\right)}
\text{,}\\
p(r,\theta,z) &
=\sum_{l=-L}^{L}\sum_{n=-N}^{N}\hat{p}(r,n,l)\cdot e^{i\left(n k_{\theta}\theta+l k_{z} z\right)}
\text{,}\\
T(r,\theta,z) &
=\sum_{l=-L}^{L}\sum_{n=-N}^{N}\hat{T}(r,n,l)\cdot e^{i\left(n k_{\theta}\theta+l k_{z} z\right)}
\text{.}
\label{eq:fourierGalerkinAnsatz}
\end{align}
Here, $k_z$ is the minimum (fundamental) axial wavenumber and therefore fixes
the axial length $\Gamma=2\pi/k_z$ of the computational domain. This is basically
the non-dimensional height of the TC cylinder in terms of the gap-width $d$, as
shown in figure~\ref{fig:tc} and explained in section~\ref{sec:controlParameters}.
In case of \nsp (see appendix~\ref{sec:nsPipe} for details), this corresponds to
length of the computational pipe domain. Similarly, $L_\theta=2\pi/k_\theta$ is
the azimuthal arc degree. The natural periodic boundary condition in $\theta$ --
\ie a full cylindrical domain -- is obtained by setting $k_{\theta}=1$. Setting
$k_\theta=4$, for instance, corresponds to one quarter of the annulus, c.f.
figure~\ref{fig:tc0040laminarTaylorCouette}. This imposes a fourfold azimuthal
symmetry and generates a solution in a symmetry subspace.
\par
The hat symbol (\eg $\hat{\vec{u}}$) in eq.~\eqref{eq:fourierGalerkinAnsatz} denotes
quantities in Fourier space (data type see section~\ref{sec:constantVariablesAndDataTypes})
and the tuple $(N, L)$ therefore determines the spectral numerical resolution in $\theta$
and $z$. The fundamental wavenumbers ($k_{\theta}\equiv\code{k\_th0}$) and
($k_{z}\equiv\code{k\_z0}$), as well as the number of Fourier modes ($N\equiv\code{m\_{th}}$)
and ($L\equiv\code{m\_{z0}}$), can be specified by the user in the \code{nsCouette.in}
input file, as described in section~\ref{sec:parameterInputFile}.
\par
The coefficients $\vec{\hat{u}}(r,l,n)$, $\vec{\hat{p}}(r,l,n)$ and
$\vec{\hat{T}}(r,l,n)$ are complex numbers known as spectral coefficients.
They contain the entire instantaneous flow field information and are
written to the binary \code{coeff\_} files, as described in
section~\ref{sec:ioCoeff}.
\par
Because the physical flow field ($\vec{u}, p, T$) is real valued,
the $\theta$-coefficients -- coming from a real-to-complex Fourier
transform -- are complex valued but complex conjugate symmetric with
respect to the zero mode. The $z$-coefficients -- coming from a
complex-to-complex Fourier transform -- do in general not exhibit any
symmetry.
in the beginning
of the azimuthal symmetry of the TC setup,
spectral coefficients corresponding to negative and positive azimuthal modes are
complex conjugated. Therefore, only half of the $\theta$ modes need to be stored
and solved for, whereas all $z$ modes need to be considered. To reduce the size
of the arrays and the number of computations, the equations are solved only for
the positive azimuthal modes and the symmetry property is enforced through the
FFT in the computations of the nonlinear terms. The total number of spectral
coefficients used in a simulation with \nsc is hence given by
$m_f = \left(m_{th}+1\right) \times 2m_{z0}$.

%TODO: add information on total number of DOF
% the effective number of DOF $N_r \times m\_f$
% the actual number of DOF, dealiasing 2/3 rule etc...
% also, the radial FD scheme schould be explaind here, in the "spatial
% discretisation" section and not somewhere else, but we merge/fix this later....

\subsection{Accuracy of the radial derivatives}
\label{sec:accuracyFD}

In \nsc the radial derivatives are approximated by high order finite differences.
The accuracy of this approximation is determined by the stencil length, \ie the number
of consecutive nodes that are used in the approximation. The stencil length in \nsc
corresponds to the parameter \code{n\_s}, which can be found in the module
\code{mod\_params.f90}. Hence, by setting the value of \code{n\_s} at compile time, the
user can modify the order of accuracy of the finite difference scheme. The only
restriction is that $\code{n\_s}\geq\num{5}$.
%, \ie the radial derivatives must be at least fourth order accurate.
The default implementation of \nsc is $\code{n\_s} = 9$.
% uses eighth order finite differences (\code{n\_s} $= 9$).
\par
Note that to keep the matrices banded the stencil length decreases towards
the boundaries (\eg one sided finite difference approximations are used at the
cylinders). However, for radial grid distributions with $\alpha \leq \num{0.5}$,
the global order is not affected by this reduction due to the clustering of nodes
near the cylinders (see figure 3 in~\cite{Shi2015}).
\par
As a final remark, we would like to stress that increasing the order of the
finite difference scheme does not impact on the code's performance. This is
because the computational cost of evaluating the radial derivatives (basically
matrix-vector multiplications) is practically negligible as compared with the
cost of the fast Fourier transforms performed during the computation of the
nonlinear terms.

\subsection{Temporal discretisation}
\label{sec:timeStepper}

Since the publication of our CAF paper~\cite{Shi2015}, quite some changes have been
implemented to \nsc to further improve its usability and performance. Notable among
these is the implementation of a new time-stepper. The governing equations are now
integrated forward in time using a predictor-corrector algorithm, which allows to
significantly increase the computational timestep size $\Delta t$ in the simulation.
The ideas for this time-stepper were borrowed from the \opf project; an open source
code for pipe flow simulations developed by Ashley Willis~\cite{Willis2017}. A
comprehensive description can be found on Ashley's web page, while the basic steps
of our implementation are outlined in the following.
\begin{itemize}

\item
In the predictor step the equations are solved explicitly to obtain a rough
approximation of the velocity field $u_1^{q+1}$. If the matrices containing
the linear and non-linear terms are denoted as $L$ and $N$ respectively, the
predictor step can be written as
\begin{align*}
\nabla P_1^{q+1} & = \nabla\cdot(Lu^q+N^q)\\
\left(\frac{1}{\delta t} - c\nabla^2\right) u_1^{q+1} & =
N^q + \left(\frac{1}{\delta t}-(1-c)\nabla^2\right) u^{q} - \nabla P_1^{q+1}
\end{align*}

\item
The velocity computed in the predictor step ($u_1^{q+1}$) is then refined in
multiple corrector steps, until a certain tolerance is reached. The iteration
for the corrector step is given by
\begin{align*}
\nabla P_{j+1}^{q+1} & = \nabla\cdot\left(Lu^q+N_j^{q+1}\right)\\
\left(\frac{1}{\delta t}-c \nabla^2\right) u_{j+1}^{q+1} & =
c N_j^{q+1} + \left(1-c\right)N^q + \left( \frac{1}{\delta t} -
\left(1-c \right) \nabla^2 \right) u^{q} - \nabla P_{j+1}^{q+1}
\end{align*}
and the tolerance is set to \code{tolerance\_dterr = 5E-5} by default. If
necessary, it can easily by changed by modifying the file \code{mod\_param.f90},
see section~\ref{sec:}

\item The coefficient $c$ defines the implicitness of the temporal integration
scheme. It is set to \code{d\_implicit = 0.5} by default and can easily be
changed at compile time by modifying the file \code{mod\_params.f90}. Note,
that the temporal scheme is of second-order accuracy if $c=0.5$.

\item The boundary conditions are imposed through influence matrices which are
computed at a pre-processing stage.

\item The user can choose between fix or variable time-step size. In the latter
case $\Delta t$ is computed as
\begin{align}
\Delta t = C \min\left(\nabla / \lvert v\rvert\right)
\text{,}
\end{align}
where $C$ is the Courant or CFL number.
%\href{https://en.wikipedia.org/w/index.php?title=Courant_number&redirect=no}
%{Courant or CFL number}.

\end{itemize}

\subsection{Feature for thermal convection}
\label{sec:thermalConvection}

You can easily switch to the optional temperature version of \nsc by simply
setting the external variable \code{CODE=TE\_CODE} when building the code.
Note, that the standard version of \nsc~-- \ie without the additional
temperature equation -- is set by default (\code{CODE=STD\_CODE}), see also
section~\ref{sec:compileTimeOptions}.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/nsCouette$ make ARCH=myPlatform clean
feldmann@darkstar:~/nsCouette/nsCouette$ make ARCH=myPlatform HDF5IO=no CODE=TE_CODE
\end{lstlisting}
\par
The equation for the temperature ($T$) is coupled to the Navier--Stokes
equations by considering a Boussinesq-like approximation that includes
centrifugal buoyancy effects. Details about the dimensionless governing
equations and the Boussinesq-like approximation are documented in Lopez
et al.~\cite{Lopez2013}. Now, the code produces additional output. For
details also see section~\ref{sec:output}. The file \code{Nusselt}
contains time series of the normalised heat transfer at the inner and
outer cylinders walls and the file \code{Temp\_energy} contains time
series of the modal energy related to the temperature field. Time series
of the temperature at the some user specified probe locations do now
appear in the files \code{probes*.dat}. The snapshot flow field files
(\code{coeff\_*} and \code{fields*.h5}) now also contain full information
about the instantaneous temperature field.
\par
In the provided version of \nsc, the flow is stratified radially by
heating the inner cylinder and cooling the outer cylinder, \ie a
negative radial temperature gradient $\sfrac{\partial T}{\partial r}$.
A descriptive example is given in the tutorial~\ref{sec:tc0073}.

\subsubsection{How to modify the source code}

The sense of the temperature gradient can be easily modified by inverting
the boundary conditions for the temperature in the file \code{mod\_timeStep.f90}
and modifying the temperature and axial velocity of the base flow accordingly
in the \code{subroutine base\_flow} contained in the file \code{mod\_InOut.f90}.
\begin{lstlisting}[language=Fortran]
! Boundary conditions for mode 0 (prescribed temperature at the cylinders)
if (abs(fk_mp%th(1,k)) <= epsilon .and. abs(fk_mp%z(1,k)) <= epsilon) then
 rhs_p(1)   = dcmplx( 0.5d0, 0d0)
 rhs_p(m_r) = dcmplx(-0.5d0, 0d0)
end if
\end{lstlisting}


\subsection{Parallelisation scheme}
\label{sec:parallelisationScheme}

Data distribution across \mpi tasks is accomplished using a one-dimensional (1d)
slab decomposition of the computational domain. This technique follows naturally and
straightforwardly from the Fourier--Galerkin ansatz we use for spatial discretisation,
see section~\ref{sec:spatialDiscretisation}: when the variables are expanded in Fourier
series, equations~\ref{eq:nse} are separated into a system of $m_f$ independent linear
equations, \ie one equation for each spectral coefficient. These equations are evenly
distributed across the number of \mpi tasks, denoted in the code as \code{np}. Each
discretised variable in \nsc is stored as a two-dimensional (2d) array, $(m\_r,m\_f)$,
with the second dimension identifying the spectral coefficients and the first dimension
its radial dependence. When the 1d slab decomposition is applied, the data is partitioned
along the second dimension of the array, $mp\_f = m\_f/np$, so that each \mpi task
contains a data chunk of size $(m\_r,mp\_f)$.
\par
Since the computation of the nonlinear terms is carried out using a pseudospectral
technique, spectral data must be transformed into data in physical space. These
transformations are performed using fast Fourier transforms (FFT), which require each
\mpi task to contain all spectral coefficients. To satisfy this requirement, while
keeping a 1d slab domain decomposition, the data, during the calculation of the nonlinear
terms, is divided along the radial direction and organised in 2d arrays of size
$(m\_f, mp\_r = m\_r/np)$. Such modification of the storage arrangement is efficiently
performed by using global data transpositions based on \code{MPI\_Alltoall} and task-local
transposes.
\par
Note that both the number of radial modes $(m\_r)$ and Fourier modes $(m\_f)$ must
be divisible by the number of \mpi tasks $(np)$, which imposes a restriction in the
selection of the numerical resolution. \nsc implements a simple technique that relaxes
partially such restriction by allowing a free selection of the number of Fourier modes.
If $m\_f$ is not divisible by $np$, $m\_f$ is automatically increased to satisfy the
divisibility condition and the additional spectral coefficients are set to zero. Hence,
the only restriction for the selection of modes in \nsc is that $m\_r$ must be divisible
by $np$.
\par
Since in turbulent simulations the number of radial modes is typically much smaller
than the total number of Fourier modes ($m\_r << m\_f$), the number of \mpi tasks that
can be used in our 1d slab decomposition is limited by the value of $m\_r$. Consequently,
if only this paralellisation strategy were applied, the achievable parallel speedup with
respect to a serial code version would be at most $m\_r$. In order to increase the parallel
scalability beyond this limit, an additional \omp parallelisation layer has been introduced.
In the parts of the code where the solution of the linear equations is computed, \omp
threads allow to parallelise over the $mp\_f$ spectral coefficients contained within each
\mpi task. That is, if \code{n\_threads} is the number of \omp threads within each \mpi
task, each thread will solve $mp\_f/n\_threads$ linear equations to obtain the spectral
coefficients. In the part where the non-linear terms are computed, some coarse-grain
parallelism (\eg the scalar state variables and the components of the velocity vector can
be Fourier-transformed independently of each other and some of these computations can be
overlapped with the global transpositions) can be exploited with the \omp threads.
\par
A sketch illustrating the data distribution for this hybrid strategy is shown in
figure~\ref{fig:domdec}. The variables here are discretised using $m\_r=8$ radial
modes and $m\_f=64$ spectral coefficients, and the simulation is run on a machine
with 64 cores.
\begin{figure}[htb]
\includegraphics[width=1.00\linewidth]{figures/hybrid_par.pdf}
\caption{Sketch of the one-dimensional domain decomposition of the
two-dimensional numerical grid of eight radial (r) points times 64 Fourier
(f) modes. Like in the code a linearised index is used to label the
Fourier modes in the $z$ and $\theta$ directions.}
\label{fig:domdec}
\end{figure}
Data chunks resulting from the \mpi slab decomposition are indicated by the
letter  P followed by a number. Note that the number of \mpi tasks has been set to
eight (P0\dots P7), which is the maximum possible value in this example ($np=m\_r$).
There are also four \omp threads per \mpi task which are indicated by the letter T
followed by a number (T0\dots T3). The subgroups of data that are handled by each
thread are shown in different colours. With the choice of $np=8$, $mp\_f=64/8=8$
linear equations are solved within each \mpi task (\code{mod\_timestep.f90}), and
only one radial point, $mp\_r=8/8=1$, is contained within each \mpi task in the
transposed layout (\code{mod\_nonlinear.f90}). By using four \omp threads per \mpi
task in this example, an additional factor of four in concurrency and hence a total
theoretical parallel speedup of \num{32} is gained for the linear terms. In the
transposed layout, by contrast, since a single radial point is computed by each
\mpi task, the theoretical speedup would be limited by eight. Nevertheless, as
indicated above, additional concurrency for the four \omp threads per \mpi task is
exposed by:
\begin{enumerate}
\item
Computing the Fourier transforms and derivative computations for each scalar
variable and the individual components of vectors, respectively, independently
in parallel.
\item
Handling the global \mpi transposition for some of the variables while others
are still being computed.
\end{enumerate}
Finally, we would like to note that an alternative strategy to the use of \omp is
to perform a 2d \mpi domain decomposition. This strategy has been used in a number
of similar codes for the simulation of fluid flows (\eg Ashley Willis' \opf
\cite{Willis2017}, John Gibson's \cf, Simpson, et cetera). We conducted a comparative
study of the performance and scalability of two pipe flow codes with similar algorithmic
formulation but one implementing a 2d \mpi domain decomposition~\cite{Willis2017} and
the other a hybrid \mpi-\omp strategy (\nsp). We observed that both strategies may be
beneficial depending on the type of problem under consideration. For example, for big
setups (\ie large $m\_r$ and large $m\_f$) typical for high Reynolds number turbulence,
our hybrid strategy exhibited a much better performance and scalability. However, in
problems where extreme long pipes are considered (\ie large $m\_f$) at rather low
Reynolds (\ie small $m\_r$), a 2d \mpi decomposition in $r$ and $z$ turned out to be
more efficient than our hybrid strategy. Overall, when confronted with a 2d \mpi-only
domain decomposition, we believe that our hybrid parallelisation scheme is a good
compromise between simplicity of the implementation and achievable parallel scalability,
which fits perfectly with the technological trend of multi-core processors with ever
increasing core counts and stagnating per-core performances.



\section{Tutorials}
\label{sec:tutorials}

\subsection{Prerequisites}
\begin{itemize}
\item
Get a recent \gnuplot installation to quickly inspect the time series output
interactively and to easily produce some decent line plots using the \code{*.gpl}
script files, which come with this tutorials. The scripts provided here were
generated and tested using version \code{5.0}.
% In appendix~\ref{app:} you can find a sample instruction which will help you to install a recent version.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette$ gnuplot --version
gnuplot 5.0 patchlevel 3
\end{lstlisting}
\item
For more advanced plotting and data analysis a recent \python installation can be
quite helpful. Some of this tutorials come with a few basic \code{*.py} script files
which can be used as a starting point for your further work.
\item
For three-dimensional flow field visualisation we recommend you to install \paraview
(Section~\ref{sec:paraview}) or \visit (Section~\ref{sec:visit}).
\item
A proper \LaTeX~installation might also be useful \eg to compile this manual or to convert
the \code{*.eps} output from \gnuplot to proper \code{*.pdf} figures.
\end{itemize}

\subsection{Laminar Taylor-Couette flow in the stable regime}
\label{sec:tc0040}
To start off, we will first run a simulation of Taylor-Couette flow in the
stable regime, \ie at a very low Reynolds number. This allows us to perform our
first DNS on a regular laptop in less than a minute. Moreover, we can readily
compare the results to what we expect from theory. Go to your working directory,
copy the first Taylor-Couette tutorial case from the repository and inspect the
parameter input file (\code{nsCouette.in}) using your favourite  text editor
(\eg \code{vi}, \code{emacs}, \code{kate}, \code{gedit} and such).
\begin{lstlisting}[language=bash]
feldmann@darkstar:~$ cd ~/nsCouette
feldmann@darkstar:~/nsCouette$ cp -r ../nscouette/tutorials/tc0040 .
feldmann@darkstar:~/nsCouette$ cd tc0040
feldmann@darkstar:~/nsCouette/tc0040$ vi nsCouette.in
\end{lstlisting}
We choose to keep the outer cylinder wall stationary by setting $\Reo=0$ and further
choose a very low rotation speed of the inner cylinder by setting $\Rei=50$. The
corresponding part in the input file \code{nsCouette.in} looks like this.
\begin{lstlisting}[language=Fortran]
&parameters_physics
Re_i = 50.0d0   ! inner cylinder Reynolds number
Re_o =  0.0d0   ! outer cylinder Reynolds number
/
\end{lstlisting}
All the other parameters in this namelist (\code{parameter\_physics}) are
irrelevant for our first tutorials. They will be discussed later in
section~\ref{sec:tc0073}, when we turn to thermal convection. See also
section~\ref{sec:parameterInputFile} and \ref{sec:listOfParameters} for
an exhaustive description of the parameter input file. Due to this very
small Reynolds number, a very coarse spatial resolution should be sufficient
to produce accurate results. Here, we choose $M=16$ radial grid points in
physical space and $N=L=4$ Fourier modes in azimuthal ($\theta$) and axial
($z$) direction, respectively. The corresponding entries in the namelist
\code{paramters\_grid} are shown here.
\begin{lstlisting}[language=Fortran]
&parameters_grid
m_r  = 16            ! M radial grid points
m_th =  4            ! N azimuthal Fourier modes
m_z0 =  4            ! L axial Fourier modes
/
\end{lstlisting}
This choice corresponds to a total of $n_r \times n_\theta \times n_z=M \times
(2N+1) \times (2L+1) = \num{16} \times \num{9} \times \num{9}$ significant
physical grid points, while the nonlinear terms are actually evaluated on $M
\times (3N+1) \times (3L+1) = \num{16} \times \num{13} \times \num{13}$ physical
grid points for dealiasing purposes. For further details, definitions and
notations regarding the spatial discretisation scheme of \nsc, you might want to
have a look at our CAF paper \cite{Shi2015} around page \num{3}. \par We want
the simulation to run for \num{8000} timesteps. And the size of the computational
timestep $\Delta t$ should be dynamically adapted to automatically guarantee
numerical stability during time integration. Both can be controlled using the
relevant keywords in the namelist \code{parameters\_timestep}.
\begin{lstlisting}[language=Fortran]
&parameters_timestep
numsteps    = 8000       ! Number of computational timesteps
variable_dt = T          ! Use variable (T) or fixed (F) timestep size
/
\end{lstlisting}
Integrating the Navier-Stokes equations forward in time is a typical initial value
problem. Therefore, one remaining issue to discuss -- before we finally run our first
DNS -- is choosing proper initial conditions (Section~\ref{sec:initialConditions})
to start from. Since we do not have any suitable flow field data at hand, we choose
the most simple -- and compared to a real world laboratory set-up also the most
similar -- condition to start with. As can be seen in the parameter input file, we
choose to start our simulation from scratch and prescribe a zero-velocity field
($u_{r} = u_{\theta} = u_{z} = 0$) as initial flow state ($t=0$), just like the
resting fluid in the experimental set-up just before you switch on the cylinder
rotation.
\begin{lstlisting}[language=Fortran]
&parameters_control
restart = 0           ! Start from scratch (0) or restart from checkpoint (1,2)
/

&parameters_initialcondition
ic_tcbf = F ! Set Taylor-Couette base flow (T) or resting fluid (F), only when restart = 0
ic_pert = T ! Add perturbation on top of base flow (T) or not (F), only when restart = 0
ic_p(1, :) = 4.0d-2, 0, 1 ! 1st perturbation: amplitude and wavevector (a1, k_th1, k_z1)
ic_p(2, :) = 6.0d-3, 1, 0 ! 2nd perturbation: amplitude and wavevector (a2, k_th2, k_z2)
/
\end{lstlisting}
Additionally, the initial zero-velocity flow field is superimposed by a finite
amplitude perturbation to test whether the code actually captures the well-known
stable behaviour of the Taylor-Couette system for such a small Reynolds number.
When thinking of a real world laboratory set-up, perturbations could model residual
motions from filling the tank, mechanical vibrations from the drive, small density
or temperature gradients and such. Up to $l=6$ independent perturbations can be
prescribed by specifying the tuple $(a_l, k_{\theta,l}, k_{z,l})$, representing
the respective perturbation amplitude and wave vector. \par If you have already
compiled the code (Section~\ref{sec:buildingTheCode}), you now simply have to copy
the executable to your case directory and start the simulation by prompting the
executable and passing the parameter input file to the standard input.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0040$ cp ../nscouette/darkstar/nsCouette.x .
feldmann@darkstar:~/nsCouette/tc0040$ ./nsCouette.x < nsCouette.in
\end{lstlisting}
Further details and different ways to start a simulation can be found in
section~\ref{sec:runningTheCode}. Now \nsc runs on one single \mpi task and
throws its log output directly to the terminal. You can watch the progress
of the simulation step by step, which should usually finish in less than a
minute (Here after \SI{4.15}{\second}). The few first and last lines of the
log should look something like this.
\begin{lstlisting}[language=bash]
starting NSCouette (a88e2a11) using 1 MPI tasks, 8 OpenMP threads/task
TASKS: 1 THREADS: 8 THIS: 0 Host:darkstar Cores: 0 0 0 0 0 0 0 0
  step=   100  dt=   1.1100000000000001E-004
  step=   200  dt=   1.2321000000000003E-004
  step=   300  dt=   1.3676310000000004E-004
  ...
  step=  7700  dt=   1.8704145521610010E-004
  step=  7800  dt=   1.8704145521610010E-004
  step=  7900  dt=   1.8704145521610010E-004
 written hdf5/xdmf files to disk: fields_tc0040_00008000.{h5,xmf}
  step=  8000  dt=   1.8704145521610010E-004
 written coeff file to disk: coeff_tc0040.00008000
 written coeff file to disk: coeff_tc0040.00008000
------------------------------------------------------------------------------------
Total number of computed timesteps: 00008000
Total elapsed WCT since start up:  4.15s
Average elapsed WCT per timestep w/o coeff io (min, mean, max): 0.0005s, 0.0005s, 0.0005s
feldmann@darkstar:~/nsCouette/tc0040$
\end{lstlisting}
The log tells us, that at some particular timesteps, \nsc wrote instantaneous
state files (\code{coeff*} and \code{fields*}) to disk, as described in
section~\ref{sec:output}. Listing the content of our case directory by typing
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0040$ ls
coeff_tc0040.00002000	    coeff_tc0040.00008000	    ke_total     probe05.dat
coeff_tc0040.00002000.info  coeff_tc0040.00008000.info	ke_z	     probe06.dat
coeff_tc0040.00004000	    fields_tc0040_00008000.h5	probe01.dat  torque
coeff_tc0040.00004000.info  fields_tc0040_00008000.xmf	probe02.dat
coeff_tc0040.00006000	    ke_mode			            probe03.dat
coeff_tc0040.00006000.info  ke_th			            probe04.dat
\end{lstlisting}
reveals, that next to the four Fourier coefficient files (Section~\ref{sec:ioCoeff})
and the one physical flow field file (Section~\ref{sec:iohdf5}) there are also some
time series data files. They contain temporal information about the kinetic energy
(\code{ke\_*}), the velocity at six individual probe locations (\code{probe0*.dat}),
and the torque Nusselt number (\code{torque}). Let's have a look at the torque
data first, which is an integral system quantity. By typing something like
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0040$ head -n 5 torque
1.1100000000000E-003     1.4343271052595E+001     1.4059654954111E-009   ...
2.2200000000000E-003     1.0042605705170E+001    -4.1595016094334E-009   ...
3.3300000000000E-003     8.1720439469609E+000     2.6556245818254E-008   ...
4.4400000000000E-003     7.0899641989767E+000    -3.7680014324448E-009   ...
5.5500000000000E-003     6.3560048662009E+000    -6.2320501650089E-008   ...
feldmann@darkstar:~/nsCouette/tc0040$ tail -n 5 torque
1.4642139302435E+000     1.0000008374531E+000     9.9999896083642E-001   ...
1.4660843447956E+000     1.0000008220272E+000     9.9999897984950E-001   ...
1.4679547593478E+000     1.0000008069813E+000     9.9999899851429E-001   ...
1.4698251738999E+000     1.0000007921586E+000     9.9999901683719E-001   ...
1.4716955884521E+000     1.0000007776289E+000     9.9999903482444E-001   ...
\end{lstlisting}
the first/last five lines of this simple time series text file are dumped to the
terminal. The first column represents the advancing physical time ranging from
the initial state ($t=0$) we have specified, up to the very end ($t\approx\num{1.5}$)
of this simulation run. The next two columns represent the instantaneous
non-dimensional torque \Nuomi and \Nuomo integrated over the inner and outer
cylinder wall, respectively. Data columns four and five (not shown here), represent
the friction Reynolds number (\ReTau) at the inner and outer cylinder wall. We can
already see here, that $\Nuomo=0$ in the initially resting flow field, since the
outer wall as well as the adjacent fluid are at rest. Since the inner cylinder
wall starts to move at $t=0$ at a constant speed while the adjacent fluid have
been initialised at rest, the torque $\Nuomi$ exerted to the fluid by the inner
cylinder wall, abruptly takes rather high values. At the end of the simulation,
both torque values have reached a value of one. This makes total sense, since the
flow state at this low Reynolds number is dominated by viscosity and should
therefore recover the analytical solution for laminar Taylor-Couette flow. The
torque Nusselt number is defined as the actual torque normalised by the torque of
the laminar flow state (See % \S\ref{sec:tc} and
\eg \cite{Brauckmann2016} around page \num{426} for further details).
Therefore, our torque results compare favourably with theoretical predictions.
\par More can be seen by making a simple time series plot using \eg
\gnuplot. In this tutorial case directory you can find a ready-to-use
script to generate the plot shown in figure \ref{fig:tc0040torque} by simply
typing
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0040$ gnuplot torque.gpl
feldmann@darkstar:~/nsCouette/tc0040$ okular torque.pdf &
feldmann@darkstar:~/nsCouette/tc0040$ ls torque*
torque	torque.eps  torque.gpl	torque.pdf
\end{lstlisting}
and opening the generated \code{*.pdf} or \code{*.eps} figure using your
favourite document  viewer (\eg \code{okular}, \code{evince}, \code{acroread}
and alike).
\begin{figure}[htb]
\centering
\includegraphics[scale=1.0, trim=0mm 0mm 0mm 6mm, clip=true]{figures/tc0040/torque}
\caption{Temporal evolution of the torque at the inner and outer cylinder
wall in our first Taylor-Couette tutorial \code{tc0040} with $\Rei=\num{50}$.
The torque is expressed in a non-dimensional way using a type of a Nusselt
number, which relates the actual torque to the torque of the laminar
Taylor-Couette analytical solution.}
\label{fig:tc0040torque}
\end{figure}
We see that the inner and outer torque values both monotonically converge to the
theoretically predicted value of one after roughly $\orderof\left(1\right)$
viscous time units. This again makes total sense, since this is approximately
the time span it should take for the presence of the driving inner wall to
propagated to the opposing wall (at a distance $d=1$) solely by the action of
viscosity. % and take effect everywhere in the flow domain.
\par
Next, we take a
look at the time series output of the velocity vector at some particular probe
locations in the flow field, which will give us more local insights into the
flow, where the torque provided us with rather global informations. By simply
typing
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0040$ gnuplot probes.gpl
feldmann@darkstar:~/nsCouette/tc0040$ okular probes.pdf &
\end{lstlisting}
we generate a plot as shown in figure \ref{fig:tc0040probes}.
\begin{figure}[htb]
\centering
\includegraphics[scale=1.00]{figures/tc0040/probes}
\caption{Temporal evolution of the velocity components at individual probe
locations in our first Taylor-Couette tutorial \code{tc0040} with $\Rei=\num{50}$.
The streamwise velocity component ($u_{\theta}$) is shown
at three different radial locations ($r_{i}\le r\le r_{o}$) and converges
everywhere to the theoretically predicted values (left). The absolute value of
the two cross-stream components ($|u_r|$ and $|u_z|$) is shown exemplarily at
one radial location ($r=\num{7.0}$).}
\label{fig:tc0040probes}
\end{figure}
We can easily see that the streamwise (azimuthal) velocity component
($u_{\theta}$) converges nicely to the theoretically predicted values everywhere
in the flow domain ($r_{i}\le r\le r_{o}$). As expected, both cross-stream
velocity components ($|u_r|$ and $|u_z|$) decay to zero. The initial finite
amplitude perturbations which we imposed do not survive in the linearly sable
regime and the analytical solution for laminar Taylor-Couette flow is nicely
reproduced/recovered. Information about the analytical Taylor-Couette solution can be
found in the \code{probes.gpl} file by typing \eg
\begin{lstlisting}[language=Gnuplot]
sed -n '18, 33 p' probes.gpl

# Taylor-Couette set-up
Re_i = 50.0 # inner cylinder Reynolds number
Re_o =  0.0 # outer cylinder Reynolds number
eta = 0.868 # raddii ratio

# Analyitcal Taylor-Couette solution
nutc = 1.0 # Taylor-Couette analytical torque (Nusselt number)
c1 = (Re_o - eta * Re_i) / (1 + eta)
c2 = (eta * (Re_i - eta*Re_o)) / ((1 - eta) * (1 - eta**2.0))
r1 = 6.741192272578147 # radial probe location from file header
r2 = 7.023493344123748 # radial probe location from file header
r3 = 7.410322878937004 # radial probe location from file header
utc1 = c1*r1 + c2/r1  # Taylor-Couette analytical velocity
utc2 = c1*r2 + c2/r2  # Taylor-Couette analytical velocity
utc3 = c1*r3 + c2/r3  # Taylor-Couette analytical velocity
\end{lstlisting}
or \eg in \cite{Shi2015} on page \num{5} and in \cite{Brauckmann2016} around
page \num{424}. The particular radial probe locations for which the analytical
solution is needed to produce figure \ref{fig:tc0040probes}, can be found in the
header of the \code{probe0*.dat} files by typing
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0040$ head -n 10 probe01.dat
# Time series data from probe 01 on rank 00000
# Radial location n_r = 00005 n_rp = 0005 r =  6.741192272578147E+00
# Azimuthal location n_th = 00003 th =  2.617993877991494E-01
# Axial location n_z = 00003 z =  6.000000000000010E-01
# Time, u_r, u_th, u_z
9.9900000000000E-004  4.9755347543047E-002 -5.0697872196875E-001  -3.9262289916098E-001
2.1090000000000E-003  4.9251475018859E-002  2.0445331579843E-003  -3.7513595580142E-001
3.2190000000000E-003  4.8415330734597E-002  1.4934842744334E+000  -3.5808738661681E-001
4.3290000000000E-003  4.7373835221887E-002  3.3163726265500E+000  -3.4181516483879E-001
5.4390000000000E-003  4.6167965547437E-002  5.1781059897116E+000  -3.2637614051804E-001
\end{lstlisting}
In case you are interested in time series data at different probe locations, you
can modify the coordinates and the sampling rate in the parameter file using
your favourite text editor
\begin{lstlisting}[language=Fortran]
feldmann@darkstar:~/nsCouette/tc0040$ vi nsCouette.in
&parameters_output
dn_prbs   = 10  ! output interval [steps] for time series data at probe locations
prl_r(1)  = 0.20d0 ! radial probe locations (0 < r/d < 1)
prl_r(2)  = 0.50d0
prl_r(3)  = 0.80d0
prl_r(4)  = 0.20d0
prl_r(5)  = 0.50d0
prl_r(6)  = 0.80d0
prl_th(1) = 0.25d0 ! azimuthal probe locations (0 < th/L_th < 1)
prl_th(2) = 0.25d0
prl_th(3) = 0.25d0
prl_th(4) = 0.75d0
prl_th(5) = 0.75d0
prl_th(6) = 0.75d0
prl_z(1)  = 0.25d0 ! axial probe locations (0 < z/L_z < 1)
prl_z(2)  = 0.25d0
prl_z(3)  = 0.25d0
prl_z(4)  = 0.75d0
prl_z(5)  = 0.75d0
prl_z(6)  = 0.75d0
/
\end{lstlisting}
and restart (Section~\ref{sec:checkpoint}) or rerun the simulation.
\par
Since we have only looked at particular points in the flow field so far,
figure \ref{fig:tc0040laminarTaylorCouette} shows a three-dimensional
view on the entire computational domain ($(r_{i}\le r\le r_{o}) \times
(\num{0}\le\theta\le L_{\theta}) \times (\num{0}\le z\le L_{z})$) we have
chosen for our first test case.
\begin{figure}[htb]
\centering
\includegraphics[width=1.00\textwidth]{figures/tc0040/laminarTaylorCouette.png}
\caption{The depicted volume represents the entire computational domain
($(\num{6.576}\le r\le\num{7.576}) \times (\num{0}\le\theta\le\sfrac{2\pi}{6})
\times (\num{0}\le z\le\num{2.4})$) we have chosen for our first DNS; \ie only
one sixth of a full cylinder with a very small axial height. The colour-coding
represents the only non-zero velocity component ($u_{\theta}$), which is
obviously invariant with respect to $\theta$ and $z$ and only varies with
respect to the wall-normal location $r$ according to the analytical solution
for laminar Taylor-Couette flow.}
\label{fig:tc0040laminarTaylorCouette}
\end{figure}
To keep the simulation simple and quick, we restrict ourselves to only a segment of
a full real-world Taylor-Couette cylinder set-up. Here we chose one sixth of the
full azimuth ($L_{\theta}=\sfrac{2\pi}{k_{\theta,0}}=\sfrac{2\pi}{6}$) and only a
very small height of ($L_{z}=\sfrac{2\pi}{k_{z,0}}=\sfrac{2\pi}{2.618}=\num{2.4}$),
as can be specified by setting
\begin{lstlisting}[language=Fortran]
&parameters_grid
k_th0 = 6.0d0               ! azimuthal fundamental wavenumber
k_z0  = 2.61799387799149d0  ! axial fundamental wavenumber
eta   = 0.868d0             ! inner to outer radii aspect ratio
\
\end{lstlisting}
in the namelist \code{parameters\_grid} in the parameter input file.
The colour-coding in figure~\ref{fig:tc0040laminarTaylorCouette} represents
the only non-zero velocity component
\begin{align}
\vec{u}=
\begin{bmatrix}
u_{r}\\ u_{\theta}\\u_{z}
\end{bmatrix}
=
\begin{bmatrix}
0\\ f(r)\\0
\end{bmatrix}
\text{,}
\end{align}
which is obviously invariant with respect to the azimuth $\theta$ and the axial
location $z$. It only varies with respect to the wall-normal location $r$ and
decreases monotonically from the inner cylinder wall at $r=r_{i}=\num{6.576}$,
which is driven at a speed of $\num{50}\sfrac{d}{\nu}$, down to zero at the outer
cylinder wall ($r=r_{o}=\num{7.576}$), which is kept stationary. This pseudo-colour
representation of the flow field can be easily reproduced using  \paraview
and loading the state file \code{la\-mi\-nar\-Tay\-lor\-Cou\-ette.pvsm} which comes with this
tutorial. Further details on how to visualise flow fields can be found in
section~\ref{sec:paraview} or section~\ref{sec:visit}, in case you prefer the software
\visit.
\par
Last but not least we want to have a look at the time series data for the
kinetic energy. The two files \code{ke\_th} and \code{ke\_z} provide integral
kinetic energies for each discrete Fourier mode ($n_{\theta}$ and $l_{z}$,
respectively) at each timestep. So in our case, both files contain six
columns of time series data: the  first one being the physical time $t$ and
the next five being the integral kinetic energy in mode number
$\num{0}\le n_{\theta}\le N=\num{4}$ and $\num{0}\le l_{z}\le L=\num{4}$,
respectively. The kinetic energy per mode is an integral quantity in the sense,
that it is integrated in both cases over the radial direction ($r$) and also
over either the axial ($z$) or the azimuthal ($\theta$) direction, respectively,
depending on which quantity you are looking at.
By simply typing
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0040$ gnuplot keThZ.gpl
feldmann@darkstar:~/nsCouette/tc0040$ okular keThZ.pdf &
\end{lstlisting}
we generate a plot as shown in figure~\ref{fig:tc0040keThZ}.
\begin{figure}[htb]
\centering
\includegraphics[scale=1.00]{figures/tc0040/keThZ.pdf}
\caption{Temporal evolution of the azimuthally (left) and axially (right)
dependent kinetic energy contained in each discrete azimuthal ($n_{\theta}$)
and axial ($l_{z}$) mode for our first Taylor-Couette tutorial \code{tc0040}
with $\Rei=\num{50}$. See also page 3 in \cite{Shi2015} for definitions and
details on the spatial discretisation scheme of \nsc. As expected, the non-zero
kinetic energy introduced by the finite amplitude perturbation in the initial
velocity field decays monotonically to practically zero in all modes and never
increases again.}
\label{fig:tc0040keThZ}
\end{figure}
Since there is no relevant energy content in any of the modes larger than the zero
mode (which represents the mean value in the respective direction), there is no
variation of the flow field neither in $\theta$ nor in $z$ direction. And there is
obviously also no periodic or chaotic variation with respect to time at this low
Reynolds number. Actually, the kinetic energy in all modes decays to practically
zero. The amount of energy introduced to the initial conditions by the finite amplitude
perturbations is monotonically damped out by the dominating effect of viscosity and it
never increases again. You can easily double-check this for larger times by simply
restarting (Section \ref{sec:checkpoint}) the simulation and let it run for another
\num{8000} or more timesteps. Although the dynamics at this low Reynolds number is
rather boring and also trivial as well as expected, these per mode energies are a
very convenient and important measure to track and analyse the dynamical behaviour
of the Taylor-Couette system in general. However, have in mind, that for more
interesting (\ie increasing) Reynolds numbers -- and thus finer spatial resolution -- these
files progressively become huge and are not so easy to handle anymore. One option
could be to increase the sampling frequency for these specific output. This can be done
using the respective keyword in the namelist \code{parameters\_output} in the parameter input file.
\begin{lstlisting}[language=fortran]
feldmann@darkstar:~/nsCouette/tc0040$ vi nsCouette.in
&parameters_output
dn_ke = 1000   ! timestep output interval for modal kinetic energy
/
\end{lstlisting}
Additionally, the highest wave numbers represent the spatial resolution
of the simulation, see section~\ref{sec:spatialDiscretisation}. Plots
like these in figures~\ref{fig:tc0040keThZ}, \ref{fig:tc0041keThZ} and
\ref{fig:tc0042keThZ} become very handy as a first indicator for the quality
of the spatial resolution. To
resolve all relevant flow scales, the trailing coefficients of the
spectral expansions should be smaller than the leading coefficients
by at least four or five orders of magnitude. Here, the highest wave
numbers only show numerical noise at a very low level
($\orderof\left(\num{e-30}\right)$) but no physically relevant energy
content. Hence, the spatial resolution in $\theta$ in $z$ direction can
be considered as sufficient.



\subsection{Taylor-vortex flow}
\label{sec:tc0041}

Above a certain Reynolds number -- slightly depending on the exact geometry and
boundary conditions -- the Taylor-Couette flow becomes unstable and gives rise
to Taylor vortices. In a next step, we want to increase the Reynolds number to
the unstable regime and see whether \nsc reproduces this well known Taylor-vortex
state. Go back to your working directory, copy the second Taylor-Couette tutorial
case from the repository and inspect the parameter file (\code{nsCouette.in})
using your favourite text editor.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~$ cd ~/nsCouette
feldmann@darkstar:~/nsCouette$ cp -r ../nscouette/tutorials/tc0041 .
feldmann@darkstar:~/nsCouette$ cd tc0041
feldmann@darkstar:~/nsCouette/tc0041$ vi nsCouette.in
\end{lstlisting}
We triple the rotation speed of the inner cylinder by changing the respective
keywords in the namelist \code{parameters\_physics}. Since we now expect a slightly
more complex flow field, we also have to increase the spatial resolution to
account for the spatial velocity gradients related to the pronounced vortex
in the flow field.
\begin{lstlisting}[language=Fortran]
&parameters_physics
Re_i = 150.0d0 ! inner cylinder Reynolds number
Re_o =   0.0d0 ! outer cylinder Reynolds number
/
&parameters_grid
m_r  = 24   ! M radial points
m_th =  4   ! N azimuthal Fourier modes
m_z0 =  8   ! L axial Fourier modes
/
\end{lstlisting}
This choice corresponds to a total of $n_r \times n_\theta  \times n_z=M \times
(2N+1) \times (2L+1) = \num{24} \times \num{9} \times \num{17}$ significant
physical grid points. The finer spatial resolution naturally demands a smaller
computational timestep $\Delta t$. Since $\Delta t$ will be adapted automatically
to ensure numerical stability during integration, we simply have to increase the
total number of timesteps in namelist \code{parameters\_timestep}, to end up at
roughly the same physical time $t$ as in our first tutorial case
(Section~\ref{sec:tc0040}).
\begin{lstlisting}[language=Fortran]
&parameters_timestep
numsteps    = 22000   ! Number of computational timesteps
variable_dt = T       ! Use variable (T) or fixed (F) timestep size
/
\end{lstlisting}
Both changes will
surely increase the overall computational time necessary for our second DNS.
To save resources and to keep the tutorials small, we now choose start our
second simulation from a flow state similar to the one we ended up with in our
first tutorial (Section~\ref{sec:tc0040}). We do this by prescribing the analytical
Taylor-Couette velocity profile as initial condition and again add a finite
amplitude perturbation on top of it.
\begin{lstlisting}[language=Fortran]
feldmann@darkstar:~/nsCouette/tc0041$ vi nsCouette.in
&parameters_initialcondition
ic_tcbf = T ! Set Taylor-Couette base flow (T) or resting fluid (F), only when restart = 0
ic_pert = T ! Add perturbation on top of base flow (T) or not (F), only when restart = 0
ic_p(1, :) = 4.0d-2, 0, 1 ! 1st perturbation: amplitude and wavevector (a1, k_th1, k_z1)
ic_p(2, :) = 6.0d-3, 1, 0 ! 2nd perturbation: amplitude and wavevector (a2, k_th2, k_z2)
/
\end{lstlisting}
To run this tutorial, you now simply have to copy the same executable as before
(Section~\ref{sec:tc0040}) to the new case directory and start the simulation by
prompting the executable and passing the new parameter input file to the standard input.
Further details and different ways to start a simulation can be found in
section~\ref{sec:runningTheCode}.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0041$ cp ../nscouette/darkstar/nsCouette.x .
feldmann@darkstar:~/nsCouette/tc0041$ ./nsCouette.x < nsCouette.in
...
  step=       19000  dt=   6.6666666666666670E-005
  step=       20000  dt=   6.6666666666666670E-005
  step=       21000  dt=   6.6666666666666670E-005
 written hdf5/xdmf files to disk: fields_tc0041_00022000.{h5,xmf}
  step=       22000  dt=   6.6666666666666670E-005
 written coeff file to disk: coeff_tc0041.00022000
 written coeff file to disk: coeff_tc0041.00022000
------------------------------------------------------------------------------------
Total number of timesteps: 00022000
Total elapsed WCT:    26.96s
Elapsed WCT per timestep w/o coeff io (min, max, average):   0.0012s,    0.0012s,    0.0012s
\end{lstlisting}
Now \nsc runs on one single \mpi task and throws its log output directly
to the terminal. You can watch the progress of the simulation step by step,
which should usually finish in less than a minute (Here after \SI{26.96}{\second}).
\par
Let's continue with first having a look at the final flow state.
Figure~\ref{fig:tc0041vectorPlot} shows a three-dimensional representation
of the instantaneous flow field at the last timestep from our simulation run.
\begin{figure}
\includegraphics[width=1.00\textwidth]{figures/tc0041/vectorPlot.png}
\caption{Three-dimensional flow field visualisation using  \paraview.
Shown are iso-contours (red/blue: $u_{r}=\pm\num{4}$, grey:
$u_{\theta}=\num{75}$, yellow/green: $u_{z}=\pm\num{4}$) and a vector
representation of the cross-stream velocity components. Stationary
Taylor-vortex flow state at $\Rei=\num{150}$.}
\label{fig:tc0041vectorPlot}
\end{figure}
As already mentioned above, the higher Reynolds number now gives rise
to a spatially more complex state with a secondary flow ($u_{r,z}\neq\num{0}$)
forming toroidal vortices. As shown in figure~\ref{fig:tc0041vectorPlot},
alternating regions of positive (red) and negative (blue) $u_r$ push high-speed
fluid from the rotating inner cylinder towards the stationary outer cylinder;
And Vice versa for low-speed fluid. This can be easily seen by the bent
iso-surface (grey) representing the average streamwise velocity
($u_{\theta}=\sfrac{\left(\Rei+\Reo\right)}{2}$) between the cylinders.
To fulfil mass conservation, these regions are flanked by four alternating regions of
axial up (green) and down (yellow) wash. This well-known phenomenon is a so-called
Taylor-Vortex (TV), which is more thoroughly discernible through the vector representation
of the two cross-stream velocity components $u_{r}$ and $u_{z}$, which are also included
in figure~\ref{fig:tc0041vectorPlot}. You can easily create such a plot with \paraview
by loading the corresponding \code{*.h5} file (Section \ref{sec:iohdf5}), performing a
coordinate transformation (Section \ref{sec:paraview}) and playing around with the
\code{contour}, \code{slice} and \code{glyph} filters. Loading the provided state file
(\code{taylorVortexIsoContourVectorPlot.pvsm}) into \paraview might also be helpful to
learn creating such plots.
\par
The Taylor-Vortex state -- which is reached after a certain transient phase -- is
invariant in $\theta$ (axisymmetric) and also invariant in $t$ (stationary). The
first one can be easily seen from the discussed flow field representation shown
in figure~\ref{fig:tc0041vectorPlot}. The latter one can be easily seen from the
time series of $u_{\theta}$ shown in figure~\ref{fig:tc0041probes}.
\begin{figure}[htb]
\centering
\includegraphics[scale=1.00]{figures/tc0041/probes}
\caption{Temporal evolution of the velocity components at individual probe
locations in our second Taylor-Couette tutorial \code{tc0041} with $\Rei=\num{150}$.
The streamwise velocity component ($u_{\theta}$) is shown
at three different radial locations ($r_{i}\le r\le r_{o}$) and converges
everywhere to a constant value different from the theoretically predicted values for
the simple Taylor-Couette flow state. The values of the two cross-stream components
($u_r$ and $u_z$) also converges to constant but non-zero value, as shown exemplarily
at one radial location ($r=\num{7.0}$).}
\label{fig:tc0041probes}
\end{figure}
The streamwise velocity component recorded at three different radial
locations ($r_{i}\le r\le r_{o}$) converges everywhere to a constant
value, which is, however, different from the one theoretically predicted
for the simpler Taylor-Couette flow state. Note, that this time we
started our simulation from the analytical Taylor-Couette flow field
and not from a resting fluid.
\par
The values of the two cross-stream components ($u_r$ and $u_z$) also converge to constant
but non-zero values after a certain transient phase, see figure~\ref{fig:tc0041probes}. Due
to the presence of a Taylor-Vortex pair, stronger velocity gradients occur near the walls.
These stronger gradients lead to higher shear-stresses and thus to higher torque values
compared to pure Taylor-Couette flow state, as can be seen in figure~\ref{fig:tc0041torque}.
\begin{figure}[htb]
\centering
\includegraphics[scale=1.0, trim=0mm 0mm 0mm 0mm, clip=true]{figures/tc0041/torque}
\caption{Temporal evolution of the torque at the inner and outer cylinder
wall in our second Taylor-Couette tutorial \code{tc0041} with $\Rei=\num{150}$.
The torque is expressed in a non-dimensional way using a type of a Nusselt
number, which relates the actual torque to the torque of the laminar
Taylor-Couette analytical solution.}
\label{fig:tc0041torque}
\end{figure}
Since momentum and energy must be conserved, a minimal requirement
for sufficient spatial resolution is that the torque at the inner cylinder
matches that of the outer cylinder. The time series provided by \nsc and
plots like figures~\ref{fig:tc0041torque} allow to easily monitor the
evolution of these quantities and hence check this resolution requirement.
It can be easily seen, that both torque values match very well. For a more
quantitative analysis, an additionally python script is also provided
(\code{nsCouette/postprocessing/python}) to manipulate (plot, average,
compare, etc) the torque time series. See also the thermal convection
tutorials~\ref{sec:tc0073}.
The plots shown in figures~\ref{fig:tc0041probes}, \ref{fig:tc0041torque},
\ref{fig:tc0041keThZ} can be easily reproduced with the ready-to-use
\gnuplot scripts provided in this tutorial.
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/tc0041$ gnuplot probes.gpl
feldmann@darkstar:~/nsCouette/tc0041$ gnuplot torque.gpl
feldmann@darkstar:~/nsCouette/tc0041$ gnuplot keThZ.gpl
feldmann@darkstar:~/nsCouette/tc0041$ okular probes.pdf torque.pdf keThZ.pdf &
\end{lstlisting}
\begin{figure}[htb]
\centering
\includegraphics[scale=1.00]{figures/tc0041/keThZ.pdf}
\caption{Temporal evolution of the azimuthally (left) and axially (right)
dependent kinetic energy contained in each discrete azimuthal ($n_{\theta}$)
and axial ($l_{z}$) mode for our second Taylor-Couette tutorial \code{tc0041}
with $\Rei=\num{150}$.}
\label{fig:tc0041keThZ}
\end{figure}

\subsection{Wavy vortex flow}
\label{sec:tc0042}

Figrues \ref{fig:tc0042torque} to \ref{fig:tc0042keThZ} show the results for
a wavy vortex flow state simulation, which can be reproduced analogue to the
detailed description in tutorial \ref{sec:tc0040} and \ref{sec:tc0041}.\begin{figure}[htb]
\centering
\includegraphics[scale=1.0]{figures/tc0042/torque}
\caption{Temporal evolution of the torque at the inner and outer cylinder
wall in our third Taylor-Couette tutorial \code{tc0042} with $\Rei=\num{458.1}$.
The torque is expressed in a non-dimensional way using a type of a Nusselt
number, which relates the actual torque to the torque of the laminar
Taylor-Couette analytical solution.}
\label{fig:tc0042torque}
\end{figure}
\begin{figure}[htb]
\centering
\includegraphics[scale=1.00]{figures/tc0042/probes}
\caption{Temporal evolution of the velocity components at individual probe
locations in our third Taylor-Couette tutorial \code{tc0042} with $\Rei=\num{458.1}$.
The streamwise velocity component ($u_{\theta}$) is shown
at three different radial locations ($r_{i}\le r\le r_{o}$) and converges
everywhere to the theoretically predicted values (left). The absolute value of
the two cross-stream components ($|u_r|$ and $|u_z|$) is shown exemplarily at
one radial location ($r=\num{7.0}$).}
\label{fig:tc0042probes}
\end{figure}
\begin{figure}[htb]
\centering
\includegraphics[scale=1.00]{figures/tc0042/keThZ.pdf}
\caption{Temporal evolution of the azimuthally (left) and axially (right)
dependent kinetic energy contained in each discrete azimuthal ($n_{\theta}$)
and axial ($l_{z}$) mode for our third Taylor-Couette tutorial \code{tc0042}
with $\Rei=\num{458.1}$. See also page 3 in \cite{Shi2015} for definitions and
details on the spatial discretisation scheme of \nsc. As expected, the non-zero
kinetic energy introduced by the finite amplitude perturbation in the initial
velocity field decays monotonically to practically zero in all modes and never
increases again.}
\label{fig:tc0042keThZ}
\end{figure}
More tutorials will follow in the future!






\subsection{Thermal convection}
\label{sec:tc0073}

The flow of air between a hot rotating cylinder and a cooled stationary
cylindrical enclosure is a simple model to investigate heat transfer in
rotating machines~\cite{Howey2012}. At low rotation rates and small temperature differences,
the heat transfer is purely conductive and the flow has only an azimuthal
component. In this simple case, the governing equations admit a simple
analytic solution, termed basic state, whose temperature and velocity
profiles depend only on the radial coordinate, see \eg equations (2) to (4)
in \cite{Lopez2015}. Heat transfer can be enhanced by either increasing the
speed of the inner cylinder (forced convection), or by increasing the
temperature difference (natural convection). In both cases, the basic state exhibits
a sequence of distinct instabilities leading to turbulent heat transfer~\cite{Lopez2015}. A
measure of the efficiency is given by the Nusselt number \Nui, which is the
ratio of total heat transfer at the inner cylinder, normalised by the heat transfer
of the purely conductive basic state at the same temperature difference.
\par
Here, we want to reproduce this behaviour in a series of three short DNS runs.
First, you have to build the temperature version (\code{TE\_CODE}) of \nsc,
as described in section~\ref{sec:buildingTheCode}.
Once this is done, go to your working directory, copy the first thermal convection
tutorial case from the repository
\begin{lstlisting}[language=bash]
cd ~/nsCouette
cp -r ../nscouette/tutorials/tc0073 .
cd tc0073
vi nsCouette.in
\end{lstlisting}
and inspect the parameter file (\code{nsCouette.in}) using your favourite
text editor.



In this tutorial case directory you can find two ready-to-use scripts to
generate the time series plots shown in figure \ref{fig:compareGrassass} by
simply typing
\begin{lstlisting}[language=bash]
gnuplot probeCompare.gpl
gnuplot torqueCompare.gpl
okular probeComapre torqueCompare.pdf &
\end{lstlisting}
and opening the generated \code{*.pdf} figures using your favourite document
viewer (\eg \code{okular}, \code{evince}, \code{acroread} and alike).


Now we want to increase the effect of thermal convection by increasing the
temperature difference between the inner and outer cylinder wall. In \nsc
this can be done by specifying a larger Grashof number. Go back to your working
directory, copy the next convection tutorial case and inspect the input file
\begin{lstlisting}[language=bash]
cd ~/nsCouette
cp -r ../nscouette/tutorials/tc0075 .
cd tc0075
vi nsCouette.in
\end{lstlisting}
using your favourite text editor. Note, that we now have increased \Gr to \num{4000}
and that we have also increased the axial resolution, since we expect a slightly more
complex flow state. Moreover, we now want to run the simulation for another
\num{400000} timesteps (instead of \num{200000} as before) to end up with roughly
the same physical time span, since we now expect a slightly smaller $\Delta t$ due to
larger velocities and a finer grid resolution:
\begin{lstlisting}[language=fortran]
vi nsCouette.in
&parameters_grid
...
m_z0 = 32  ! L axial Fourier modes => 2*m_z0+1 grid points (axial)
/
&parameters_physics
...
Gr = 4000.0d0 ! Grashof number Gr = Ra/Pr [TE_CODE only]
/
&parameters_timestep
...
numsteps = 400000   ! number of computational timesteps
/
&parameters_output
...
fBase_ic = 'tc0075' ! identifier for coeff_ (checkpoint) and fields_ (hdf5) files
/
&parameters_control
...
restart = 1   ! initial conditions, start from: 0=scratch, 1,2=checkpoint
/
\end{lstlisting}


To specify the initial conditions, create a symbolic link to the last flow field snapshot
from the former DNS at the lower \Gr
\begin{lstlisting}[language=bash]
ln -s ../tc0073/coeff_tc0073.00200000 coeff_tc0075.00200000
cp ../tc0073/coeff_tc0073.00200000.info restart
\end{lstlisting}
and also create a file \code{restart} from the corresponding \code{*.info} file.
The file \code{restart} has to be slightly modified to account for the new case/base
name
\begin{lstlisting}[language=bash]
vi restart
&parameters_restart
...
fbase_ic = tc0075
/
&parameters_info
...
/
\end{lstlisting}
using your favourite text editor.
\par
Figure~\ref{fig:compareGrass} summarises the results of three DNS with increasing
temperature difference at a fixed rotation rate, using \nsc.
\begin{figure}[htb]
\centering
\includegraphics[width=1.00\textwidth]{figures/compareGr.pdf}
\caption{Different flow states for increasing Grass number when restarted
the simulation from a former run. Temporal evolution of the torque at the inner
cylinder wall and the streamwise (azimuthal) velocity component at one mid-gap
probe location. Iso-surfaces of the mean Temperature ($T=0$) colour-coded by the
inwards (blue) and outwards (red) directed wall-normal (radial) velocity component.}
\label{fig:compareGrass}
\end{figure}
Scripts (\gnuplot) and state files (\paraview) to create the shown plots
are provided in the tutorial case directories.
\par
The first DNS was initialized by applying small single harmonic disturbances to
the basic state at $\Rei=\num{50}$ and $\Rayleigh=\num{2130}$.
Fig.~\ref{fig:compareGrass}a shows that initially $\Nui\approx\num{1}$,
corresponding to purely conductive heat transfer. However, after roughly two
viscous time units, a sharp increase of \Nui is observed, indicating that the
basic state has become unstable. This is confirmed by the time-series of
$u_{\theta}$ at a fixed mid-gap probe location in the computational domain, see
Fig.~\ref{fig:compareGrass}b. The final state of this run
($t\approx\num{3.5}\sfrac{d^2}{\nu}$) is visualized in
Fig.~\ref{fig:compareGrass}c, which shows a three-dimensional rendering of a
$T=0$ iso-surface generated with \code{ParaView}. The temperature surface is
color-coded by inward/outward (blue/red) facing values of the wall-normal
velocity component $u_r$. By saving several snapshots, a movie can be easily
produced, which reveals a spiral flow pattern rotating at a constant speed
like a barber pole. This explains why the $u_{\theta}$ signal reaches a state
where it is periodic in time: the spiral pattern passes repeatedly through the
probe location at which the velocity is recorded without changing its shape.
This also explains why the integral heat flux (\Nui) remains constant. The
second and third DNS were initialized with the final state of the former runs
and by increasing the Rayleigh number to $\Rayleigh=\num{2840}$ and \num{3550},
respectively. The time series and the final states of these runs are also
shown in Fig.~\ref{fig:compareGrass}. They reveal that the flow state undergoes a
sequence of transitions to different flow states with increasing
spatio-temporal complexity as \Rayleigh increases.
\par
As already mentioned in the tutorials before (e.g. \ref{sec:tc0041}),
the time series output of the modal kinetic energy (figures~\ref{fig:tc0040keThZ},
\ref{fig:tc0041keThZ}, and \ref{fig:tc0042keThZ}) can be used to check the
spatial resolution of a simulation. Another minimal/necessary requirement
for sufficient spatial resolution is that the torque at the inner cylinder
matches that of the outer cylinder. The same holds for the Nusselt number.
This is because momentum and energy must be conserved in the system.
The time series provided by \nsc allow to easily monitor the
evolution of these quantities and hence check this resolution requirement.
For a quantitative analysis, a python script is provided average, plot and
compare the torque time series, see figure~\ref{fig:tc0076torqueNusseltCompare}.
\begin{figure}[htb]
\centering
\includegraphics[width=0.49\textwidth]{figures/tc0076/torque.pdf}
\includegraphics[width=0.49\textwidth]{figures/tc0076/Nusselt.pdf}
\caption{Temporal evolution of the torque Nusselt number and the Nusselt
number at the inner and outer cylinder. The black dashed lines represent
time averaged values at the inner and outer cylinder walls. The length of
the black dashed lines represent the averaging interval. Python scripts
to reproduce such plots are shipped with the code.}
\label{fig:tc0076torqueNusseltCompare}
\end{figure}
\begin{lstlisting}[language=bash]
feldmann@fsmcluster:~/nsCouette/tc0076$ python torque.py
Read time series data from file torque
Data from t = 7.3975195320694 to t = 12.97130416704
Temporal averaging from ta0 = 10.000006327682  to ta1 = 12.97130416704
Averaged inner torque <ti>_ta = 1.2710938766304294
Averaged outer torque <to>_ta = 1.2731075611260696
Absolute torque difference  o-i    =  0.002013684495640211
Relative torque difference (o-i)/o =  0.0015842138276822885
Written file torque.pdf
feldmann@fsmcluster:~/nsCouette/tc0076$ python Nusselt.py
Read time series data from file Nusselt
Data from t = 7.3975195320694 to t = 12.97130416704
Temporal averaging from ta0 = 10.000006327682  to ta1 = 12.97130416704
Averaged inner torque <ti>_ta = 1.1474836522546679
Averaged outer torque <to>_ta = 1.1476308708970335
Absolute torque difference  o-i    =  0.00014721864236566518
Relative torque difference (o-i)/o =  0.00012829694094238135
Written file Nusselt.pdf
feldmann@fsmcluster:~/nsCouette/tc0076$
\end{lstlisting}



\section{Frequently asked questions (FAQ)}
\label{sec:faq}



\begin{enumerate}
\item
\begin{itemize}
\item[Q:] How do I compile the code for multiple processors?
\item[A:] There is no special compile-time-option (Section~\ref{sec:compileTimeOptions}) to
generate such things like a serial or parallel executable. The number of processors can
freely be selected at \textit{runtime}, as described in the section~\ref{sec:runningTheCode}.
\end{itemize}

\item
\begin{itemize}
\item[Q:] Why?
\item[A:] Because!
\end{itemize}

\end{enumerate}




\appendix



\section{List of parameters}
\label{sec:listOfParameters}

Starting with \nsc version \code{1.0}, the file \code{mod\_params.f90} needs
no more editing before building the code. Instead, most of the settings and
parameters can be specified at run-time as described in section~\ref{sec:parameterInputFile}.
So in general, \nsc has to be build only once on a system. The same executable
can be used for all your simulations within one project, as long as you don't
want to change something in the code or want to switch to \eg simulations with
thermal convection. The latter one is described in section~\ref{sec:thermalConvection}
and in the tutorials section~\ref{sec:tc0073}. Another example for when you need to
modify source files and rebuild the code, is changing the parameters of the time
stepper, which is detailed in section~\ref{sec:timeStepper}.
\par
For reference and completeness, we here print the \code{mod\_params.f90} file
from a pre-\code{1.0} code version; also in order to document the geometrical
and numerical parameters that were used for the example of Figure 5 in our CAF
paper~\cite{Shi2015}. The variable names in the source code are more or less
compatible with the symbols and notation used in our CAF paper.
\begin{lstlisting}[language=Fortran]
INTEGER(KIND=4),PRIVATE   :: ir
!--- Mathematical constants
REAL(KIND=8)   ,PARAMETER :: epsilon = 1D-10  ! numbers below it = 0
REAL(KIND=8)   ,PARAMETER :: PI = ACOS(-1d0)  ! pi = 3.1415926...
COMPLEX(KIND=8),PARAMETER :: ii = DCMPLX(0,1) ! Complex i = sqrt(-1)

!--- Spectral parameters
INTEGER(KIND=4),PARAMETER :: m_r  = 32  ! Maximum spectral mode (> n_s-1)
INTEGER(KIND=4),PARAMETER :: m_th = 16  ! Number of Fourier modes
INTEGER(KIND=4),PARAMETER :: m_z0 = 16
INTEGER(KIND=4),PARAMETER :: m_z = 2*m_z0
INTEGER(KIND=4),PARAMETER :: m_f  = (m_th+1)*m_z ! Total number of Fourier modes
REAL(KIND=8)   ,PARAMETER :: k_th0= 6.D0   ! Minimum azimuthal wavenumber
REAL(KIND=8)   ,PARAMETER :: k_z0 = 2*PI/2.4  ! Minimum axial wavenumber

!--- Physical parameters
INTEGER(KIND=4),PARAMETER :: n_r  = m_r ! Number of grid points
INTEGER(KIND=4),PARAMETER :: n_th = 2*m_th
INTEGER(KIND=4),PARAMETER :: n_z  = m_z
INTEGER(KIND=4),PARAMETER :: n_f  = n_th*n_z  ! Number of points in Fourier directions
REAL(KIND=8)   ,PARAMETER :: len_r  = 1d0  ! Physical domain size
REAL(KIND=8)   ,PARAMETER :: len_th = 2*PI/k_th0
REAL(KIND=8)   ,PARAMETER :: len_z  = 2*PI/k_z0
REAL(KIND=8)   ,PARAMETER :: eta = 8.68d-1 ! Radii ratio r_i/r_o
REAL(KIND=8)   ,PARAMETER :: r_i = eta/(1-eta)   ! Inner radius
REAL(KIND=8)   ,PARAMETER :: r_o = 1/(1-eta)  ! Outer radius
REAL(KIND=8)   ,PARAMETER :: r(n_r) = (/(((r_i+r_o)/2 &
- COS(PI*ir/(n_r-1))/2), ir=0,(n_r-1))/) ! Chebyshev distribution for radial points
REAL(KIND=8)   ,PARAMETER :: th(n_th) = (/(ir*len_th/n_th,ir=0,n_th-1)/)
REAL(KIND=8)   ,PARAMETER :: z(n_z) = (/(ir*len_z/n_z,ir=0,n_z-1)/)
REAL(KIND=8)   ,PARAMETER :: gap = 3.25d0  ! gap size in cm
REAL(KIND=8)   ,PARAMETER :: gra = 980  ! gravitational acceleration in g/cm**3
REAL(KIND=8)   ,PARAMETER :: nu = 1.01d-2  ! kinematic viscosity in cm**2 /s

!--- Time stepper
REAL(KIND=8), PARAMETER :: d_implicit = 0.51d0  ! implicitness
REAL(KIND=8), PARAMETER :: tolerance_dterr = 5.0d-5   ! tolerance for corrector step

!--- MPI & FFTW parameters
INTEGER(KIND=4), PARAMETER :: root = 0   ! Root processor
INTEGER(KIND=4), PARAMETER :: fftw_nthreads = 1
LOGICAL  , PARAMETER :: ifpad = .TRUE.   ! If apply '3/2' dealiasing

!--- Finite differences parameters
INTEGER(KIND=4),PARAMETER :: n_s = 9 ! Leading length of stencil

!--- Defaults for runtime parameters
REAL(KIND=8) :: Courant = 0.25d0
INTEGER(KIND=4) :: print_time_screen = 250
LOGICAL   :: variable_dt= .true.
REAL(kind=8) :: maxdt = 0.01d0
INTEGER(KIND=4) :: numsteps = 100000 ! Number of timesteps
INTEGER(KIND=4) :: dn_coeff = 5000   ! Output coefficients every nth step
INTEGER(KIND=4) :: dn_ke = 100 ! Output energy every nth step
INTEGER(KIND=4) :: dn_vel = 100   ! Output velocity every nth step
INTEGER(KIND=4) :: dn_Nu = 100 ! Output Nusselt every nth step
INTEGER(KIND=4) :: dn_hdf5 = 1000 ! Output HDF5 every nth step
\end{lstlisting}




















\section{Configure and build additional software manually}
\label{app:selfBuildLibraries}

\subsection{Build \code{zlib}}
\label{sec:buildzlib}

This is a detailed description of how to manually build \code{zlib}
libraries on our local \code{fsmcluster} at the
\href{https://www.zarm.uni-bremen.de/en/}{ZARM} institute. This is required
for installing \hdf, as described in section~\ref{sec:buildhdf}

\begin{lstlisting}[language=bash]
# Brief comments on how to install zlib locally on fsmcluster@zamr, which is
# needed for HDF5, which is needed for NetCDF with netcdf-4 capabilities.
#
# Daniel Feldmann, 18th January 2017
#
# Download and unpack to some directory of your choice, \eg ~/zlib/build
tar xfvz zlib-1.2.11.tar.gz
cd zlib-1.2.11/
#
#
# Load Intel compiler suite available on fsmclsuter
module purge
module load Intel/PSXE2017
source /home/centos/Intel/PSXE2017/bin/ifortvars.sh intel64
source /home/centos/Intel/PSXE2017/bin/compilervars.sh intel64
source /home/centos/Intel/PSXE2017/bin/iccvars.sh intel64
#
# Check compiler version
module list
mpiicc --version
#
# Use Intel C compiler for the following steps, whatever icc or mpiicc, is not
# important as far as I know...
# export CC=icc
export CC=mpiicc
export CFLAGS="-O3 -xHost -ip -mcmodel=medium"
#
# Configure zlib to be installed into some place of you wish, \eg as follows
mkdir ~/zlib/zlib-1.2.11
./configure --prefix=/home/feldmann/zlib/zlib-1.2.11
#
# Build, test and install zlib via
make
make check
make install
#
# Do not forget to unset environment varibles, which might mess up future builds
unset CC
unset CFLAGS
#
# You may need to add the path to the newly installed library to the
# LD_LIBRARY_PATH environment variable if that lib directory is not searched by
# default. \eg put the following line to your ~/.bashrc
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/feldmann/zlib/zlib-1.2.11/lib
#
# Done!
\end{lstlisting}

This is a detailed description of how to manually build \code{zlib}
libraries on my local desktop machine, which might be useful as
a rough guide line. This is required
for installing \hdf, as described in section~\ref{sec:buildhdf}
\begin{lstlisting}[language=bash]
# Brief comments on how to install zlib locally on darkstar@zarm, which is
# needed for HDF5, which is needed for NetCDF with netcdf-4 capabilities.
#
# Daniel Feldmann, 6th January 2017
#
tar xfvz zlib-1.2.11.tar.gz
cd zlib-1.2.11/
#
#
# Install openmpi and GNU C compiler if not already installed, both easily
# available via the package manager
sudo apt-get install openmpi-bin openmpi-common libopenmpi1.10 libopenmpi-dev mpi-default-bin mpi-default-dev
sudo apt-get install gcc gfortran g++
#
# Check compiler version
which mpicc
mpicc --version
#
# Use GNU C compiler for the following steps, whatever gcc or mpicc, is not
# important as far as I know...
# export CC=gcc
export CC=mpicc
export CFLAGS='-O3 -mcmodel=medium'
#
# Configure zlib to be installed into some place you wish, \eg as follows
mkdir ~/zlib/zlib-1.2.11
./configure --prefix=~/zlib/zlib-1.2.11
#
# Build, test and install zlib via
make
make check
make install
#
# Do not forget to unset environment varibles, which might mess up subsequent builds
unset CC
unset CFLAGS
#
# You may need to add the path to the newly installed library to the
# LD_LIBRARY_PATH environment variable if that lib directory is not searched by
# default. \eg put the following line to your ~/.bashrc
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/feldmann/zlib/zlib-1.2.11/lib
#
# Done!
\end{lstlisting}



\subsection{Build \hdf}
\label{sec:buildhdf}
This is a detailed description of how to manually build \mpi-parallel \hdf
libraries with \fortran interfaces on our local \code{fsmcluster} at the
\href{https://www.zarm.uni-bremen.de/en/}{ZARM} institute. This requires
a \code{zlib} installation, as described in section~\ref{sec:buildzlib}

\begin{lstlisting}[language=bash]
#
# Download and unpack to some directory of your choice, \eg ~/hdf5/build/.
tar xvf hdf5-1.10.0-patch1.tar
cd hdf5-1.10.0-patch1
#
# Load Intel compiler suite
module purge
module load Intel/PSXE2017
source /home/centos/Intel/PSXE2017/bin/ifortvars.sh intel64
source /home/centos/Intel/PSXE2017/bin/compilervars.sh intel64
source /home/centos/Intel/PSXE2017/bin/iccvars.sh intel64
#
# Check compiler version
module list
mpiicc --version
mpiifort --version
#
# Define parallel Intel compilers to be used
export CC=mpiicc
export CCP="mpiicc -E"
export FC=mpiifort
#
# Set compiler flags for larger file size support and also optimisation level 3
export CFLAGS="-O3 -mcmodel=medium -xHost -ip"
export FCFLAGS="-O3 -mcmodel=medium -xHost -ip"
#
# Specify path to locally installed zlib library
export LIBS="-lz"
export LDFLAGS="-L/home/feldmann/zlib/zlib-1.2.11/lib"
export CPPFLAGS="-I/home/feldmann/zlib/zlib-1.2.11/include/"
#
# Configure HDF5 to be build for parallel use with fortran interface, as well as
# the use of the local installation of zlib and to be installed into some
# place you wish, \eg as follows
mkdir ~/hdf5/hdf5-1.10.0-patch1
./configure --enable-parallel --enable-fortran --with-zlib=/home/feldmann/zlib/zlib-1.2.11 --prefix=/home/feldmann/hdf5/hdf5-1.10.0-patch1
#
# Build, test and install HDF5 as follows, while building and testing take quite
# some time... (enough for lunch and/or coffee)
make
make check
make install
#
# During building a lot of warnings come up like 'non-pointer conversion from
# "int" to "char" may lose significant bits' Absolutely no idea if this might
# be a problem...
#
# Do not forget to unset environment varibles, what might mess up future builds
unset CC CPP FC
unset CFLAGS FCFLAGS CPPFLAGS LDFLAGS LIBS
#
# You may need to add the path to the newly installed library to the
# LD_LIBRARY_PATH environment variable if that lib directory is not searched by
# default. \eg put the following line to your ~/.bashrc
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/feldmann/hdf5/hdf5-1.10.0-patch1/lib
#
# Done!
\end{lstlisting}

This is a detailed description of how to manually build \mpi-parallel \hdf
libraries with \fortran interfaces on my local desktop computer using
\code{gcc}. This requires a \code{zlib} installation, as described in
section~\ref{sec:buildzlib}

\begin{lstlisting}[language=bash]
 Brief comments on how to install HDF5 locally for parallel use and fortran
# interface for the use with openpipeflow on darkstar@zarm.uni-bremen.de
# HDF5 is also required for NetCDF with netcdf-4 capabilities.
#
# Daniel Feldmann, 6th February 2017
#
# 1. Download and unpack to some directory of your choice, \eg ~/hdf5/build/.
tar xvf hdf5-1.10.0-patch1.tar
cd hdf5-1.10.0-patch1
#
# 2. Make sure zlib version 1.2.5 or later is installed (do not confuse with
# libzc library)
ls /*/*zlib* /*/*/*zlib*
echo $LD_LIBRARY_PATH | grep --color=auto zlib
#
# 3. Check GNU C and fortran compiler versions
which mpicc
mpicc --version
which mpif90
mpif90 --version
#
# 4. Set GNU C and fortran compiler, parallel (openmpi) versions
export CC=mpicc
export FC=mpif90
#
# 5. Set compiler flags for larger file size support and also optimisation
# level 3 for the fortran compiler
export CFLAGS="-mcmodel=medium"
export FCFLAGS="-mcmodel=medium -O3"
#
# 6. Configure HDF5 to be build for parallel use with fortran interface, as well
# as the use of the local installation of zlib and to be installed into some
# place you wish, \eg as follows
mkdir ~/hdf5/hdf5-1.10.0-patch1
./configure --enable-parallel --enable-fortran --with-zlib=/home/feldmann/zlib/zlib-1.2.11 --prefix=/home/feldmann/hdf5/hdf5-1.10.0-patch1
#
# 7. Build, test and install HDF5 as follows, while building and testing take quite
# some time... (enough for lunch and/or coffee). Note that the parallel tests
# will only succede on a parallel file system
make
make check
make install
#
# During building a lot of warnings come up like 'non-pointer conversion from
# "int" to "char" may lose significant bits' Absolutely no idea if this might
# be a problem...
#
# 8. Do not forget to unset environment varibles, what might mess up future
# builds
unset CC FC
unset CFLAGS FCFLAGS
#
# 9. You may need to add the path to the newly installed library to the
# LD_LIBRARY_PATH environment variable if that lib directory is not searched by
# default. \eg put the following line to your ~/.bashrc
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/feldmann/hdf5/hdf5-1.10.0-patch1/lib
# Done!
\end{lstlisting}

\section{Code variant for pipe flow (\nsp)}
\label{sec:nsPipe}

The code \nsp merges the numerical formulation of \opf~\cite{Willis2017}, an
open source code to simulate pipe flow, with the hybrid parallel strategy of \nsc,
creating a highly scalable solver to simulate pipe flow at large values of the
Reynolds number. The source code structure of \nsp was conceived to follow that of
\nsc as closely as possible. Both codes share the same hardware and software
requirements (see sections~\ref{sec:hardwarePrerequisites} and
\ref{sec:softwarePrerequisites}). Likewise, \nsp can be compiled following the
steps described in section~\ref{sec:buildingTheCode}. Note, however, that the
executable obtained after compilation is in this case named \code{nsPipeFlow.x}.
Also from a user perspective, \nsp works in the same way as \nsc, and so the reader
is encouraged to read in detail all sections in this user guide and, in particular,
follow the tutorials in section~\ref{sec:tutorials}, before using \nsp.
\par
Here, we will only briefly discuss those details that are specific to \nsp,
with emphasis placed on I/O parameters or operations that differ with respect to \nsc.

\subsection{Governing equations and parameters}
\label{sec:nspGoverningEquations}

We consider a fluid flowing through a straight circular pipe of constant
cross section. Fluid motion is governed by the Navier-Stokes and continuity
equations in cylindrical coordinates (eq.~\ref{eq:eq_pipe}). These are rendered
dimensionaless by choosing the pipe radius ($R$), the centreline velocity of
the laminar profile ($u_c$) and the dynamic pressure $\rho u_c^2$ as characteristic
length, velocity and pressure scales, respectively.
\begin{align}
  \nabla\cdot\vec{u} = 0
  \hspace{1em}\text{and}\hspace{1em}
  \frac{\partial\vec{u}}{\partial t} + \vec{u}\cdot\nabla\vec{u} =
  \frac{\nabla p^{\ast}}{\rho} + \frac{1}{Re}\laplacian\vec{u} + F\hat{e}_z
  \label{eq:eq_pipe}
\end{align}
The control parameter here is the dimensionless Reynolds number, $Re = \frac{u_c R}{\nu}$,
which quantifies the ratio of inertial to viscous forces. If $Re$ is small ($\sim O(100)$)
velocity fluctuations are damped by the viscous forces and the flow remains laminar.
Unlike certain regimes of Taylor-Couette flow, where turbulence is achieved after the flow
passes through a sequence of increasingly complex states, turbulence in pipes arises
suddenly. \Ie the transition is subcritical. First observations of turbulence in pipe flow
can be made at $Re \approx 1850$ where localized turbulent patches known as puffs appear
transiently. Other complex spatio-temporal dynamics such as puff splitting and
turbulence spreading (``slugs'') are observed as $Re$ increases during the
so called transitional regime ($2050 \lesssim Re \lesssim 2800$). Persistent
space filling turbulence occurs eventually at $Re \gtrsim 3000$.
\par
The term $F\hat{e}_z$ in eq.~\eqref{eq:eq_pipe} denotes the volumetric
force driving the fluid motion. There are two ways to drive motion in pipes.
\begin{enumerate}
\item Pressure driven flow. A constant pressure gradient between inlet and outlet
is imposed. Under this condition the wall shear stress is fixed but the
bulk (average) velocity and therefore the Reynolds number
will change throuhout the simulation.
\item Flow rate driven flow. A constant flow rate is imposed,
\ie the bulk velocity is constant. Under this condition the Reynolds number
is constant but the wall shear stress will fluctuate.
\end{enumerate}
Both approaches are implemented in \nsp.

\subsection{Boundary conditions}
\label{subsec:bcpipe}

Equations~\eqref{eq:eq_pipe} require of boundary conditions to be solved. In \nsp,
periodic boundary conditions are imposed for both velocity and pressure in the streamwise
and azimuthal directions. In the radial direction boundary conditions have to be imposed
at the pole ($r=0$) and at the pipe wall ($r=R$). Since the equations in cylindrical
coordinates are singular at the pole this must be excluded from the radial grid.
The boundary closure is carried out by enforcing the proper symmetry of each azimuthal
Fourier mode at $r=0$. That is, if for a given azimuthal Fourier mode $n$,
a variable $X$ (pressure or velocity) is an even function in $r$, $X(r) = X(-r)$,
its radial derivative must be zero at the pole, $\partial_r X = 0$. However, if $X$ is
an odd function in $r$, $X(r) = -X(-r)$, the value of the variable at the pole must be zero,
$X = 0$. The pressure and the streamwise velocity will be even functions if
$n$ is even and odd functions otherwise. In contrast, the radial and azimuthal velocity
components will be odd functions if $n$ is even and viceversa. At the pipe wall,
no-slip boundary conditions are imposed for the velocity field and a homogeneous
Neumann boundary condition is used for the pressure,
\begin{align}
  \vec{u}(r=R, \theta, z, t) =
  \begin{bmatrix}
    0 & 0 & 0
  \end{bmatrix}^{\intercal},
  \hspace{1em}\text{and}\hspace{1em}
  \nabla p|_{wall} = 0
  \label{eq:pipe_wallBC}
\end{align}
We recall here that our solver does not require solving the pressure formally
as the velocity is later corrected to satisfy both no-slip and divergence free
boundary condition using an influence matrix technique.

\subsection{Input file}
\label{subsec:input_nspipe}

As in \nsc the simulation control parameters are defined in an input file named
\code{nsPipe.in}, which is read from the standard input in the initial stage of
the simulation. See also sections~\ref{sec:parameterInputFile} for further details.
This feature allows
users to modify the control parameters without the need of
recompiling the code. An example of an input file for \nsp is
given below.

\begin{lstlisting}[language=fortran]

&parameters_grid
m_r   = 32                ! radial points           => m_r      grid points (radial)
m_th  = 4                 ! azimuthal Fourier modes => 2*m_th+1 grid points (azimuthal)
m_z0  = 4                 ! axial Fourier modes     => 2*m_z0+1 grid points (axial)
k_th0 = 1.0d0             ! azimuthal wavenumber    => L_th = 2*pi/k_th0 azimuthal length of grid
k_z0  = 0.628318531d0     ! axial wavenumber        => L_z =  2*pi/k_z0  axial length of grid
/

&parameters_physics
Re = 100d0               ! Re
const_flux = T           ! T: constant flow rate; F: constant pressure gradient
/

&parameters_timestep
numsteps    = 20000     ! number of steps
init_dt     = 1.0d-4    ! initial size of timestep
variable_dt = T         ! use a variable (=T) or fixed (=F) timestep
maxdt       = 0.01      ! maximum size of timestep
Courant     = 0.25      ! CFL safety factor
/

&parameters_timestep
numsteps    = 20000     ! number of steps
init_dt     = 1.0d-4    ! initial size of timestep
variable_dt = T         ! use a variable (=T) or fixed (=F) timestep
maxdt       = 0.01      ! maximum size of timestep
Courant     = 0.25      ! CFL safety factor
/


&parameters_timestep
numsteps    = 20000     ! number of steps
init_dt     = 1.0d-4    ! initial size of timestep
variable_dt = T         ! use a variable (=T) or fixed (=F) timestep
maxdt       = 0.01      ! maximum size of timestep
Courant     = 0.25      ! CFL safety factor
/

&parameters_output
fBase_ic = 'DNS1'       ! identifier for coeff_ (checkpoint) and fields_ (hdf5) files
dn_coeff = 5000         ! output interval [steps] for coeff (dn_coeff = -1 disables ouput)
dn_ke    = 100          ! output interval [steps] for energy
dn_friction    = 100    ! output interval [steps] for friction parameters
dn_hdf5  = 1000         ! output interval [steps] for HDF5 output
print_time_screen = 100 ! output interval [steps] for timestep info to stdout
/

&parameters_control
restart = 0             ! initialisation mode: 0=new run, 1=restart from checkpoint (keep time), 2=restart from checkpoint (set time=0 and create new output files)
runtime = 86400         ! maximum runtime [s] for the job
/
\end{lstlisting}

Note that there are several differences with respect to the input file  in \nsc
(section~\ref{sec:parameterInputFile}). Since the physical mechanisms underlying
the flow dynamics differ between Taylor-Couette and pipe flows, the
\emph{parameter\_physics} namelist contain different parameters. In \nsp only two
physical parameters are needed to set up a simulation: the Reynolds number (\code{Re}) and
the force driving the fluid motion (\code{const\_flux}). The latter is a logical variable.
If it is set as true (T), motion is driven by a constant mass flow rate. Otherwise,
the flow is pressure driven. A new parameter \code{dn\_friction} appears in the
\emph{parameters\_output} list. It indicates the frequency at which the output file named
\code{friction} is updated. This file contains information about the bulk velocity or wall shear
stress (depending on whether pressure driven or constant mass flow rate are chosen), centreline
velocity of the mean velocity profile, friction velocity and friction factor. In addition to
the \code{friction} file, a file containing the mean velocity profile (\ie azimuthal and axially
averaged streamwise velocity) is also produced every \code{dn\_coeff} steps. Note that the
\code{dn\_probes} parameters,  used in \nsc to specify those physical locations at which time
series of velocity (or temperature) are recorded, are absent in the current version of \nsp.
This functionality will however be added in future releases.

\subsection{Initial condition}
\label{subsec:initial_condition_pipe}

As explained in section~\ref{sec:initialConditions} there exist three options to start
the simulation and these can be specified in the input file using the \code{restart} parameter.
When the initilization option \code{restart=0} is chosen, the simulation is started from
the base flow (the classical Hagen-Poiseuille profile), which is perturbed during the
initialisation phase. The disturbances added to the base flow are implemented in the subroutine
\code{pulse\_init} contained in the module \code{mod\_InOut}. By default,
\nsp implements a pair of streamwise localized rolls ($v=A(g+rg') cos(\theta) e^{-wsin^2(\pi z/L_z)}$ and
$u=A g sin(\theta) e^{-wsin^2(\pi z/L_z)}$, where $u$ and $v$ are the
radial and azimuthal velocity components, $L_z$ is the pipe axial length,
$r$,$\theta$ and $z$ are the three spatial directions in cylindrical
coordinates, $w$ fixes the axial length of the perturbation,
$g=(1-r^2)^2$,  and $A$ is the amplitude of the
disturbance (set to $0.1$ by default). This perturbation is known to produce puffs at the
transitional regime and turbulence at higher Reynolds numbers.
Nevertheless, the user may need to modify $A$ and $w$
depending on the values of $Re$ and $L_z$ considered in the simulations.
Unlike \nsc, the current implementation of \nsp does not allow
to define these parameters in the input file. Hence, changes need
to be done in the source code (\code{mod\_InOut.f90}) and the
code must be subsequently compiled.




\section{Useful notes to start working with \code{git}}
\label{app:git}

Say you work on a branch called \code{myBranch} to implement your own stuff to \nsc.
While you are working, the \code{master} branch or any other branch has been changed
by you or by any other developer. Say a bug was fixed in another part of the code.
Now, you want to incorporate this bug fix to your current status of \code{myBranch}.
One option to do this would be:
\begin{lstlisting}[language=bash]
feldmann@darkstar:~/nsCouette/nsCouette git checkout myBranch   # gets you on your branch
feldmann@darkstar:~/nsCouette/nsCouette git fetch origin  # gets you up to date
feldmann@darkstar:~/nsCouette/nsCouette git merge origin/master
\end{lstlisting}
The \code{fetch} command can be done at any point before the merge, \ie, you can swap the order
of \code{fetch} and \code{checkout}, because \code{fetch} just goes over to the named
remote (here \code{origin}) and says to it: "gimme everything you have that I don't", \ie,
all commits on all branches. They get copied to your repository, but named \code{origin/branch}
for any branch named \code{branch} on the remote.
\par
At this point you can use any viewer (\eg \code{git log}) to see "what they have" that you don't,
and vice versa. Sometimes this is only useful for Warm Fuzzy Feelings ("ah, yes, that is in fact
what I want") and sometimes it is useful for changing strategies entirely ("whoa, I don't want
THAT stuff yet").
\par
Finally, the \code{merge} command takes the given commit, which you can name as \code{origin/master},
and does whatever it takes to bring in that commit and its ancestors, to whatever branch you are on
when you run the \code{merge}. You can insert \code{--no-ff} or \code{--ff-only} to prevent a
fast-forward, or merge only if the result is a fast-forward, if you like.

\bibliographystyle{abbrv}
\bibliography{nsCouetteUserGuide.bib}

\end{document}